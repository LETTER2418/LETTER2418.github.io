<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>数据处理</title>
      <link href="/posts/9.html"/>
      <url>/posts/9.html</url>
      
        <content type="html"><![CDATA[<h1 id="数据处理"><a class="markdownIt-Anchor" href="#数据处理"></a> 数据处理</h1><h2 id="数据清洗"><a class="markdownIt-Anchor" href="#数据清洗"></a> 数据清洗</h2><blockquote><ul><li>缺失值处理<ul><li>删除缺失值</li><li>使用插值法（如线性插值、多项式插值、最近邻插值）填补缺失值。</li></ul></li><li>异常值处理<ul><li>使用统计方法（如IQR、Z-score）检测异常值。</li></ul></li></ul></blockquote><h2 id="数据转换"><a class="markdownIt-Anchor" href="#数据转换"></a> 数据转换</h2><blockquote><ul><li><strong>归一化</strong>：将数据缩放到固定范围（如[0, 1]）</li><li><strong>标准化</strong>：将数据转换为均值为0、标准差为1的分布（Z-score标准化）</li></ul></blockquote><h2 id="数据分割"><a class="markdownIt-Anchor" href="#数据分割"></a> 数据分割</h2><blockquote><ul><li>定义：将数据集划分为训练集、验证集和测试集，以便评估模型的性能。</li><li>主要方法：<ul><li>随机分割：随机将数据划分为训练集和测试集</li><li>时间序列分割：对于时间序列数据，按时间顺序划分（如前80%时间的数据作为训练集，后20%作为测试集）。</li><li>交叉验证：将数据划分为多个子集，轮流使用其中一个子集作为验证集，其余作为训练集。</li></ul></li></ul></blockquote><h2 id="数据增强"><a class="markdownIt-Anchor" href="#数据增强"></a> 数据增强</h2><blockquote><ul><li>定义：通过生成新数据来增加数据集的规模和多样性，通常用于图像、文本等领域。</li><li>主要方法：<ul><li>图像数据：旋转、缩放、翻转、添加噪声等。</li><li>文本数据：同义词替换、随机删除、随机插入等。</li><li>时间序列数据：时间偏移、噪声添加等。</li></ul></li></ul></blockquote>]]></content>
      
      
      <categories>
          
          <category> 数模 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>绘图</title>
      <link href="/posts/8.html"/>
      <url>/posts/8.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>建议保存为SVG格式，作为矢量图，在压缩时不会丢失任何数据。</p><ul><li><p>折线图 <code>plt.plot()</code></p></li><li><p>散点图 <code>plt.scatter()</code></p></li><li><p>柱状图 <code>plt.bar()</code></p></li><li><p>直方图 <code>plt.hist()</code></p></li><li><p>饼图 <code>plt.pie()</code></p></li><li><p>折线图 <code>plt.plot()</code></p></li><li><p>饼图 <code>plt.pie()</code></p></li><li><p>热力图</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sns.heatmap(data, cmap=<span class="string">&#x27;Reds&#x27;</span>, annot=<span class="literal">True</span>, fmt=<span class="string">&quot;.1f&quot;</span>, cbar=<span class="literal">True</span>, cbar_kws=&#123;<span class="string">&#x27;shrink&#x27;</span>: <span class="number">1.0</span>&#125;)</span><br></pre></td></tr></table></figure></li><li><p>饼图 <code>plt.pie()</code></p></li></ul></blockquote>]]></content>
      
      
      <categories>
          
          <category> 数模 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>回归模型</title>
      <link href="/posts/6.html"/>
      <url>/posts/6.html</url>
      
        <content type="html"><![CDATA[<h1 id="回归模型"><a class="markdownIt-Anchor" href="#回归模型"></a> 回归模型</h1><h2 id="岭回归ridge回归"><a class="markdownIt-Anchor" href="#岭回归ridge回归"></a> 岭回归(Ridge回归)</h2><blockquote><p>Ridge回归是一种线性回归模型，用于处理线性回归模型中多重共线性问题。通过在损失函数中加入L2正则化项来限制模型参数的大小，从而防止过拟合。</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 导入必要的库</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> RidgeCV</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 生成示例数据</span></span><br><span class="line">np.random.seed(<span class="number">42</span>)  <span class="comment"># 设置随机种子以确保结果可重复</span></span><br><span class="line">n_samples = <span class="number">100</span>  <span class="comment"># 样本数量</span></span><br><span class="line">n_features = <span class="number">10</span>  <span class="comment"># 特征数量</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成特征矩阵 X（100x10）</span></span><br><span class="line">X = np.random.rand(n_samples, n_features)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成真实系数（10x1）</span></span><br><span class="line">true_coefficients = np.array([<span class="number">2</span>, -<span class="number">1</span>, <span class="number">0.5</span>, <span class="number">3</span>, -<span class="number">2</span>, <span class="number">0</span>, <span class="number">1.5</span>, -<span class="number">0.5</span>, <span class="number">0</span>, -<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成目标变量 y（100x1），加入噪声</span></span><br><span class="line">y = X @ true_coefficients + np.random.randn(n_samples) * <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 划分训练集和测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 定义一组 alpha 值</span></span><br><span class="line">alphas = [<span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">1.0</span>, <span class="number">10.0</span>, <span class="number">100.0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 创建 RidgeCV 模型</span></span><br><span class="line">ridge_cv = RidgeCV(alphas=alphas, cv=<span class="number">5</span>)  <span class="comment"># 5折交叉验证</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. 训练模型</span></span><br><span class="line">ridge_cv.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6. 输出最优 alpha</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Best alpha:&quot;</span>, ridge_cv.alpha_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 7. 使用最优 alpha 进行预测</span></span><br><span class="line">y_pred = ridge_cv.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 8. 评估模型</span></span><br><span class="line">mse = mean_squared_error(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Mean Squared Error (MSE): <span class="subst">&#123;mse:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 9. 输出模型参数</span></span><br><span class="line"><span class="keyword">for</span> true_coef, estimated_coef <span class="keyword">in</span> <span class="built_in">zip</span>(true_coefficients, ridge_cv.coef_):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;True: <span class="subst">&#123;true_coef:<span class="number">.2</span>f&#125;</span>, Estimated: <span class="subst">&#123;estimated_coef:<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><h2 id="lasso回归"><a class="markdownIt-Anchor" href="#lasso回归"></a> Lasso回归</h2><blockquote><p>Lasso 回归（Least Absolute Shrinkage and Selection Operator）是一种线性回归方法，通过加入 L1 正则化项来实现特征选择和模型参数收缩。</p><p>与 Ridge 回归（L2 正则化）不同，Lasso 回归倾向于将某些系数压缩到零，从而实现特征选择。</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 导入必要的库</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LassoCV</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 生成示例数据</span></span><br><span class="line">np.random.seed(<span class="number">42</span>)  <span class="comment"># 设置随机种子以确保结果可重复</span></span><br><span class="line">n_samples = <span class="number">100</span>  <span class="comment"># 样本数量</span></span><br><span class="line">n_features = <span class="number">10</span>  <span class="comment"># 特征数量</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成特征矩阵 X（100x10）</span></span><br><span class="line">X = np.random.rand(n_samples, n_features)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成真实系数（10x1），其中一些系数为零</span></span><br><span class="line">true_coefficients = np.array([<span class="number">2</span>, -<span class="number">1</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1.5</span>, <span class="number">0</span>, <span class="number">1</span>, -<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成目标变量 y（100x1），加入噪声</span></span><br><span class="line">y = X @ true_coefficients + np.random.randn(n_samples) * <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 划分训练集和测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 定义一组候选的 alpha 值</span></span><br><span class="line">alphas = [<span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">1.0</span>, <span class="number">10.0</span>, <span class="number">100.0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 创建 LassoCV 模型</span></span><br><span class="line">lasso_cv = LassoCV(alphas=alphas, cv=<span class="number">5</span>, max_iter=<span class="number">10000</span>)  <span class="comment"># 5 折交叉验证</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. 训练模型</span></span><br><span class="line">lasso_cv.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6. 输出最优 alpha</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Best alpha:&quot;</span>, lasso_cv.alpha_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 7. 使用最优 alpha 进行预测</span></span><br><span class="line">y_pred = lasso_cv.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 8. 评估模型</span></span><br><span class="line">mse = mean_squared_error(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Mean Squared Error (MSE): <span class="subst">&#123;mse:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 8. 输出打包后的系数</span></span><br><span class="line"><span class="keyword">for</span> true_coef, estimated_coef <span class="keyword">in</span> <span class="built_in">zip</span>(true_coefficients, lasso_cv.coef_):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;True: <span class="subst">&#123;true_coef:<span class="number">.2</span>f&#125;</span>, Estimated: <span class="subst">&#123;estimated_coef:<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><h2 id="gauss回归"><a class="markdownIt-Anchor" href="#gauss回归"></a> Gauss回归</h2><blockquote><p>高斯过程回归（Gaussian Process Regression, GPR）是一种基于高斯过程理论的非参数机器学习方法。它通过将未知函数视为一个随机过程，并假设其函数值分布服从高斯分布。GPR的核心在于两个关键元素：均值函数和协方差函数。</p><p>均值函数：通常设置为常数（如0）或简单函数（如线性函数），用于描述数据的整体趋势。</p><p>协方差函数（核函数）：用于描述数据点之间的相关性，常见的核函数包括径向基函数（RBF）、Matérn核、周期核等。</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.gaussian_process <span class="keyword">import</span> GaussianProcessRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.gaussian_process.kernels <span class="keyword">import</span> RBF</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># 生成模拟数据</span></span><br><span class="line">X = np.random.rand(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">y = np.sin(<span class="number">10</span> * X) + np.random.normal(scale=<span class="number">0.1</span>, size=(<span class="number">100</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义高斯过程回归模型</span></span><br><span class="line">kernel = RBF(length_scale=<span class="number">1.0</span>, length_scale_bounds=(<span class="number">1e-4</span>, <span class="number">10.0</span>))</span><br><span class="line">gpr = GaussianProcessRegressor(kernel=kernel, alpha=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">gpr.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">X_test = np.linspace(<span class="number">0</span>, <span class="number">1</span>, <span class="number">100</span>).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">y_pred = gpr.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化</span></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">4</span>))</span><br><span class="line">plt.plot(X, y, <span class="string">&#x27;o&#x27;</span>, label=<span class="string">&#x27;Training Data&#x27;</span>)</span><br><span class="line">plt.plot(X_test, y_pred, <span class="string">&#x27;-&#x27;</span>, label=<span class="string">&#x27;Prediction&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="xgboost决策树模型"><a class="markdownIt-Anchor" href="#xgboost决策树模型"></a> XGBoost决策树模型</h2><blockquote><p>XGBoost是一种强大的集成学习算法，广泛应用于分类和回归任务。XGBoost的核心是基于决策树的集成学习模型。它通过迭代地构建多棵决策树，每棵树都试图纠正前一棵树的错误，最终将所有树的结果加权求和，得到最终的预测结果。</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBRegressor</span><br><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> plot_importance</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler, MinMaxScaler</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例数据</span></span><br><span class="line">date_range = pd.date_range(start=<span class="string">&#x27;2023-01-01&#x27;</span>, periods=<span class="number">1000</span>, freq=<span class="string">&#x27;D&#x27;</span>)</span><br><span class="line">y = np.sin(np.linspace(<span class="number">0</span>, <span class="number">100</span>, <span class="number">1000</span>)) + np.random.normal(<span class="number">0</span>, <span class="number">0.1</span>, <span class="number">1000</span>)  <span class="comment"># 实际值</span></span><br><span class="line">X = pd.DataFrame(&#123;</span><br><span class="line">    <span class="string">&#x27;date&#x27;</span>: date_range,</span><br><span class="line">    <span class="string">&#x27;day&#x27;</span>: date_range.day,</span><br><span class="line">    <span class="string">&#x27;month&#x27;</span>: date_range.month,</span><br><span class="line">    <span class="string">&#x27;year&#x27;</span>: date_range.year,</span><br><span class="line">    <span class="string">&#x27;dayofweek&#x27;</span>: date_range.dayofweek,</span><br><span class="line">    <span class="string">&#x27;lag_1&#x27;</span>: np.roll(y, <span class="number">1</span>),  <span class="comment"># 前一天的值</span></span><br><span class="line">    <span class="string">&#x27;lag_2&#x27;</span>: np.roll(y, <span class="number">2</span>)   <span class="comment"># 前两天的值</span></span><br><span class="line">    <span class="comment"># 使用lag_1和lag_2引入前一天和前两天的值作为模型的输入特征，有助于捕捉时间序列的     # 趋势和周期变化。大大提高预测准确性</span></span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除前两行，因为它们的滞后特征会导致NaN</span></span><br><span class="line">X = X[<span class="number">2</span>:]</span><br><span class="line">y = y[<span class="number">2</span>:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 标准化特征</span></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">X = scaler.fit_transform(X[[<span class="string">&#x27;day&#x27;</span>, <span class="string">&#x27;month&#x27;</span>, <span class="string">&#x27;year&#x27;</span>, <span class="string">&#x27;dayofweek&#x27;</span>, <span class="string">&#x27;lag_1&#x27;</span>, <span class="string">&#x27;lag_2&#x27;</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分训练集和测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建和训练模型</span></span><br><span class="line">model = XGBRegressor(</span><br><span class="line">    objective=<span class="string">&#x27;reg:squarederror&#x27;</span>,</span><br><span class="line">    max_depth=<span class="number">10</span>,</span><br><span class="line">    learning_rate=<span class="number">0.01</span>,</span><br><span class="line">    n_estimators=<span class="number">1000</span>,  <span class="comment"># 增加迭代次数</span></span><br><span class="line">    subsample=<span class="number">0.9</span>,       <span class="comment"># 有些数据以减少过拟合</span></span><br><span class="line">    colsample_bytree=<span class="number">0.9</span> <span class="comment"># 选择特征的子集</span></span><br><span class="line">)</span><br><span class="line">model.fit(X_train, y_train)  <span class="comment"># 不使用日期列</span></span><br><span class="line"></span><br><span class="line">_ = plot_importance(model, height=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制预测值和实际值</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>, <span class="number">6</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制实际值</span></span><br><span class="line">plt.plot(date_range[-<span class="built_in">len</span>(y_test):], y_test, label=<span class="string">&#x27;Actual&#x27;</span>, color=<span class="string">&#x27;blue&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制预测值</span></span><br><span class="line">plt.plot(date_range[-<span class="built_in">len</span>(y_test):], y_pred, label=<span class="string">&#x27;Predicted&#x27;</span>, color=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加标题和标签</span></span><br><span class="line">plt.title(<span class="string">&#x27;Actual vs Predicted&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Date&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Value&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示图表</span></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算均方误差</span></span><br><span class="line">mse = mean_squared_error(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Mean Squared Error: <span class="subst">&#123;mse&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="lightgbm模型"><a class="markdownIt-Anchor" href="#lightgbm模型"></a> LightGBM模型</h2><blockquote><p>LightGBM是由微软开发的一个高效、可扩展的梯度提升框架，广泛应用于分类、回归等任务。它在处理大规模数据集时表现尤为突出，特别适用于特征维度高和样本数量巨大的数据集。</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> lightgbm <span class="keyword">as</span> lgb</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据</span></span><br><span class="line">data = load_breast_cancer()</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建数据集</span></span><br><span class="line">train_data = lgb.Dataset(X_train, label=y_train)</span><br><span class="line">test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置参数</span></span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">&#x27;objective&#x27;</span>: <span class="string">&#x27;binary&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;metric&#x27;</span>: <span class="string">&#x27;binary_logloss&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;boosting_type&#x27;</span>: <span class="string">&#x27;gbdt&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;num_leaves&#x27;</span>: <span class="number">31</span>,</span><br><span class="line">    <span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.05</span>,</span><br><span class="line">    <span class="string">&#x27;feature_fraction&#x27;</span>: <span class="number">0.9</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">model = lgb.train(params, train_data, valid_sets=[test_data])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">y_pred = model.predict(X_test, num_iteration=model.best_iteration)</span><br><span class="line">y_pred = [<span class="number">1</span> <span class="keyword">if</span> p &gt; <span class="number">0.5</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> p <span class="keyword">in</span> y_pred]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 评估</span></span><br><span class="line">accuracy = accuracy_score(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Accuracy: <span class="subst">&#123;accuracy:<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="markov链模型"><a class="markdownIt-Anchor" href="#markov链模型"></a> Markov链模型</h2><blockquote><p>马尔可夫链）是一种数学模型，用于描述一个系统在不同状态之间转移的过程。它的核心特点是 “无记忆性”，即系统未来的状态只依赖于当前状态，而与过去的状态无关。这种性质称为 马尔可夫性质（Markov Property）。</p><p>适用于许多需要预测未来状态的领域。</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义状态空间</span></span><br><span class="line">states = [<span class="string">&quot;晴天&quot;</span>, <span class="string">&quot;雨天&quot;</span>, <span class="string">&quot;阴天&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义转移矩阵</span></span><br><span class="line">transition_matrix = np.array([</span><br><span class="line">    [<span class="number">0.6</span>, <span class="number">0.3</span>, <span class="number">0.1</span>],  <span class="comment"># 晴天的转移概率</span></span><br><span class="line">    [<span class="number">0.2</span>, <span class="number">0.5</span>, <span class="number">0.3</span>],  <span class="comment"># 雨天的转移概率</span></span><br><span class="line">    [<span class="number">0.4</span>, <span class="number">0.2</span>, <span class="number">0.4</span>]   <span class="comment"># 阴天的转移概率</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始状态</span></span><br><span class="line">current_state = <span class="string">&quot;晴天&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟马尔可夫链</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">simulate_markov_chain</span>(<span class="params">current_state, transition_matrix, states, num_steps=<span class="number">10</span></span>):</span><br><span class="line">    state_sequence = [current_state]</span><br><span class="line">    current_index = states.index(current_state)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_steps):</span><br><span class="line">        <span class="comment"># 根据当前状态和转移矩阵选择下一个状态</span></span><br><span class="line">        next_index = np.random.choice(<span class="built_in">len</span>(states), p=transition_matrix[current_index])</span><br><span class="line">        next_state = states[next_index]</span><br><span class="line">        state_sequence.append(next_state)</span><br><span class="line">        current_index = next_index</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> state_sequence</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行模拟</span></span><br><span class="line">state_sequence = simulate_markov_chain(current_state, transition_matrix, states, num_steps=<span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;状态序列:&quot;</span>, state_sequence)</span><br></pre></td></tr></table></figure><h2 id="随机森林搜索算法"><a class="markdownIt-Anchor" href="#随机森林搜索算法"></a> 随机森林搜索算法</h2><blockquote><p>随机森林搜索算法是一种基于随机森林模型的超参数优化方法，通常用于在机器学习中寻找最优的超参数组合。随机森林是一种集成学习方法，通过构建多棵决策树并综合它们的预测结果来提高模型的准确性和鲁棒性。随机森林搜索算法结合了随机森林模型和搜索策略（如网格搜索，随机搜索，贝叶斯优化），以高效地找到最佳超参数组合。</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> skopt <span class="keyword">import</span> BayesSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line">data = load_iris()</span><br><span class="line">X, y = data.data, data.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义随机森林模型</span></span><br><span class="line">model = RandomForestClassifier()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义超参数搜索空间</span></span><br><span class="line">param_space = &#123;</span><br><span class="line">    <span class="string">&#x27;n_estimators&#x27;</span>: (<span class="number">10</span>, <span class="number">200</span>),</span><br><span class="line">    <span class="string">&#x27;max_depth&#x27;</span>: (<span class="number">1</span>, <span class="number">50</span>),</span><br><span class="line">    <span class="string">&#x27;min_samples_split&#x27;</span>: (<span class="number">2</span>, <span class="number">20</span>),</span><br><span class="line">    <span class="string">&#x27;min_samples_leaf&#x27;</span>: (<span class="number">1</span>, <span class="number">10</span>),</span><br><span class="line">    <span class="string">&#x27;max_features&#x27;</span>: [<span class="string">&#x27;sqrt&#x27;</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 贝叶斯优化</span></span><br><span class="line">bayes_search = BayesSearchCV(estimator=model, search_spaces=param_space, n_iter=<span class="number">50</span>, cv=<span class="number">5</span>, scoring=<span class="string">&#x27;accuracy&#x27;</span>)</span><br><span class="line">bayes_search.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出最佳超参数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;最佳超参数:&quot;</span>, bayes_search.best_params_)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;最佳得分:&quot;</span>, bayes_search.best_score_)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 数模 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>深度学习入门</title>
      <link href="/posts/7.html"/>
      <url>/posts/7.html</url>
      
        <content type="html"><![CDATA[<h1 id="深度学习入门"><a class="markdownIt-Anchor" href="#深度学习入门"></a> 深度学习入门</h1><h2 id="感知机"><a class="markdownIt-Anchor" href="#感知机"></a> 感知机</h2><blockquote><p>感知机是一种二分类线性模型，通过输入特征的线性组合进行预测。</p><p>模型定义为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>s</mi><mi>i</mi><mi>g</mi><mi>n</mi><mo stretchy="false">(</mo><mi>w</mi><mo>⋅</mo><mi>x</mi><mo>+</mo><mi>b</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x)=sign(w\cdot x+b)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">n</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">b</span><span class="mclose">)</span></span></span></span></p><p>基本结构包括输入层、权重、偏置和激活函数。</p><p>对于训练数据集 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>y</mi><mn>1</mn></msub><mo stretchy="false">)</mo><mo separator="true">,</mo><mo stretchy="false">(</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator="true">,</mo><msub><mi>y</mi><mn>2</mn></msub><mo stretchy="false">)</mo><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mo stretchy="false">(</mo><msub><mi>x</mi><mi>N</mi></msub><mo separator="true">,</mo><msub><mi>y</mi><mi>N</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{(x_1, y_1), (x_2, y_2), \dots, (x_N, y_N)\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose">}</span></span></span></span> ，其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>是输入特征向量， <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>∈</mo><mo stretchy="false">{</mo><mo>−</mo><mn>1</mn><mo separator="true">,</mo><mo>+</mo><mn>1</mn><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">y_i \in \{-1, +1\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7335400000000001em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">{</span><span class="mord">−</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">+</span><span class="mord">1</span><span class="mclose">}</span></span></span></span>是对应的类别标签，损失函数为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo stretchy="false">(</mo><mi>w</mi><mo separator="true">,</mo><mi>b</mi><mo stretchy="false">)</mo><mo>=</mo><munder><mo>∑</mo><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>∈</mo><mi>M</mi></mrow></munder><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mi>w</mi><mo>⋅</mo><msub><mi>x</mi><mi>i</mi></msub><mo>+</mo><mi>b</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">L(w, b) = \sum\limits_{x_i \in M} y_i (w \cdot x_i + b)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">b</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.844441em;vertical-align:-1.094436em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.750005em;"><span style="top:-2.105664em;margin-left:0em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mrel mtight">∈</span><span class="mord mathnormal mtight" style="margin-right:0.10903em;">M</span></span></span></span><span style="top:-3.0000050000000003em;"><span class="pstrut" style="height:3em;"></span><span><span class="mop op-symbol small-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.094436em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.73333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">b</span><span class="mclose">)</span></span></span></span>，其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span></span></span>为所有误分类点的集合，损失函数的值与误分类点到分类超平面的距离成正比。</p><h3 id="工作原理"><a class="markdownIt-Anchor" href="#工作原理"></a> 工作原理</h3><ul><li>初始化权重</li><li>计算加权和：对于每个输入特征，计算加权和</li><li>激活函数：将加权和偏置的和输入到激活函数中，得到输出。权重<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span></span>是控制输入信号的重要性的参数，而偏置是调整神经元被激活的容易程度的参数</li><li>调整权重和偏置：根据分类结果与实际结果的差异，调整权重和偏置。权重调整公式为：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>=</mo><msub><mi>w</mi><mi>i</mi></msub><mo>+</mo><mi mathvariant="normal">Δ</mi><msub><mi>w</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">w_i=w_i+\Delta w_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.73333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord">Δ</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Δ</mi><msub><mi>w</mi><mi>i</mi></msub><mo>=</mo><mi>η</mi><mo stretchy="false">(</mo><mi>y</mi><mo>−</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo stretchy="false">)</mo><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\Delta w_i=\eta(y-\hat{y})x_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord">Δ</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">η</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.19444em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="mclose">)</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，偏置调整公式为：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi><mo>=</mo><mi>b</mi><mo>+</mo><mi>η</mi><mo stretchy="false">(</mo><mi>y</mi><mo>−</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">b=b+\eta(y-\hat{y})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">b</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.77777em;vertical-align:-0.08333em;"></span><span class="mord mathnormal">b</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">η</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.19444em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="mclose">)</span></span></span></span>，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>η</mi></mrow><annotation encoding="application/x-tex">\eta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">η</span></span></span></span> 是学习率，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi></mrow><annotation encoding="application/x-tex">y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span></span> 是实际结果， <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>y</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{y}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.19444em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span></span></span></span> 是预测结果，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 是第i个输入。权重 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span></span> 的更新方向是误分类点 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 的方向(y=1/-1)，调整超平面的法向量以减小分类错误。偏置 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">b</span></span></span></span> 的更新是调整超平面的截距，使超平面向误分类点移动。</li><li>重复上述步骤，直到模型在训练数据上达到满意的准确率或达到最大迭代次数。</li></ul><h3 id="扩展"><a class="markdownIt-Anchor" href="#扩展"></a> 扩展</h3><ul><li><p>单层感知机只能表示线性空间，而多层感知机可以表示非线性空间。</p></li><li><p>“朴素感知机”是指单层网络，指的是激活函数使用了阶跃函数的模型。“多层感知机”是指神经网络，即使用 sigmoid函数等平滑的激活函数的多层网络。</p></li></ul><h3 id="参考"><a class="markdownIt-Anchor" href="#参考"></a> 参考</h3><ul><li><a href="https://www.jianshu.com/p/9ea2ba3311c8">感知机公式推导及算法实现 </a></li></ul></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Perceptron</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, learning_rate=<span class="number">0.01</span>, n_iters=<span class="number">1000</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.learning_rate = learning_rate</span><br><span class="line">        <span class="variable language_">self</span>.n_iters = n_iters</span><br><span class="line">        <span class="variable language_">self</span>.weights = <span class="literal">None</span></span><br><span class="line">        <span class="variable language_">self</span>.bias = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">self, X, y</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        训练感知机模型</span></span><br><span class="line"><span class="string">        :param X: 输入特征矩阵，形状为 (n_samples, n_features)</span></span><br><span class="line"><span class="string">        :param y: 目标标签，形状为 (n_samples,)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        n_samples, n_features = X.shape</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.weights = np.zeros(n_features)</span><br><span class="line">        <span class="variable language_">self</span>.bias = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 确保标签为 +1 或 -1</span></span><br><span class="line">        y_ = np.where(y &lt;= <span class="number">0</span>, -<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 训练过程</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.n_iters):</span><br><span class="line">            flag = <span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> idx, x_i <span class="keyword">in</span> <span class="built_in">enumerate</span>(X):</span><br><span class="line">                <span class="comment"># 计算预测值</span></span><br><span class="line">                linear_output = np.dot(x_i, <span class="variable language_">self</span>.weights) + <span class="variable language_">self</span>.bias</span><br><span class="line">                y_pred = np.where(linear_output &gt;= <span class="number">0</span>, <span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 更新权重和偏置</span></span><br><span class="line">                <span class="keyword">if</span> y_pred != y_[idx]:</span><br><span class="line">                    flag = <span class="number">0</span></span><br><span class="line">                    update = <span class="variable language_">self</span>.learning_rate * y_[idx]</span><br><span class="line">                    <span class="variable language_">self</span>.weights += update * x_i</span><br><span class="line">                    <span class="variable language_">self</span>.bias += update</span><br><span class="line">            <span class="keyword">if</span> flag:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">if</span> _ == <span class="variable language_">self</span>.n_iters - <span class="number">1</span> <span class="keyword">and</span> flag == <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;警告：达到最大迭代次数，但仍有误分类样本&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, X</span>):</span><br><span class="line">        linear_output = np.dot(X, <span class="variable language_">self</span>.weights) + <span class="variable language_">self</span>.bias</span><br><span class="line">        <span class="keyword">return</span> np.where(linear_output &gt;= <span class="number">0</span>, <span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 生成训练数据</span></span><br><span class="line">    np.random.seed(<span class="number">42</span>)</span><br><span class="line">    X_positive = np.random.normal(loc=[<span class="number">2</span>, <span class="number">2</span>], scale=<span class="number">1</span>, size=(<span class="number">50</span>, <span class="number">2</span>))</span><br><span class="line">    y_positive = np.ones(<span class="number">50</span>)</span><br><span class="line">    X_negative = np.random.normal(loc=[-<span class="number">2</span>, -<span class="number">2</span>], scale=<span class="number">1</span>, size=(<span class="number">50</span>, <span class="number">2</span>))</span><br><span class="line">    y_negative = -np.ones(<span class="number">50</span>)</span><br><span class="line">    X_train = np.vstack((X_positive, X_negative))</span><br><span class="line">    y_train = np.hstack((y_positive, y_negative))</span><br><span class="line">    shuffle_idx = np.arange(<span class="number">100</span>)</span><br><span class="line">    np.random.shuffle(shuffle_idx)</span><br><span class="line">    X_train = X_train[shuffle_idx]</span><br><span class="line">    y_train = y_train[shuffle_idx]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 生成验证数据</span></span><br><span class="line">    X_positive_val = np.random.normal(loc=[<span class="number">2</span>, <span class="number">2</span>], scale=<span class="number">1</span>, size=(<span class="number">20</span>, <span class="number">2</span>))</span><br><span class="line">    y_positive_val = np.ones(<span class="number">20</span>)</span><br><span class="line">    X_negative_val = np.random.normal(loc=[-<span class="number">2</span>, -<span class="number">2</span>], scale=<span class="number">1</span>, size=(<span class="number">20</span>, <span class="number">2</span>))</span><br><span class="line">    y_negative_val = -np.ones(<span class="number">20</span>)</span><br><span class="line">    X_val = np.vstack((X_positive_val, X_negative_val))</span><br><span class="line">    y_val = np.hstack((y_positive_val, y_negative_val))</span><br><span class="line">    shuffle_idx_val = np.arange(<span class="number">40</span>)</span><br><span class="line">    np.random.shuffle(shuffle_idx_val)</span><br><span class="line">    X_val = X_val[shuffle_idx_val]</span><br><span class="line">    y_val = y_val[shuffle_idx_val]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 初始化感知机模型</span></span><br><span class="line">    perceptron = Perceptron(learning_rate=<span class="number">0.01</span>, n_iters=<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练模型</span></span><br><span class="line">    perceptron.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 在训练集上预测</span></span><br><span class="line">    train_predictions = perceptron.predict(X_train)</span><br><span class="line">    train_accuracy = np.mean(train_predictions == y_train)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;训练集准确率:&quot;</span>, train_accuracy)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 在验证集上预测</span></span><br><span class="line">    val_predictions = perceptron.predict(X_val)</span><br><span class="line">    val_accuracy = np.mean(val_predictions == y_val)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;验证集准确率:&quot;</span>, val_accuracy)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出权重和偏置</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Weights:&quot;</span>, perceptron.weights)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Bias:&quot;</span>, perceptron.bias)</span><br></pre></td></tr></table></figure><h2 id="神经网络"><a class="markdownIt-Anchor" href="#神经网络"></a> 神经网络</h2><blockquote><ul><li>神经网络的结构包括输入层，隐藏层(中间层)，输出层。</li></ul><h3 id="激活函数"><a class="markdownIt-Anchor" href="#激活函数"></a> 激活函数</h3><p>激活函数负责将神经元的输入映射到输出端，帮助网络学习和表示复杂的非线性关系。激活函数使用线性函数时，无法发挥多层网络带来的优势。因此，为了发挥叠加层所带来的优势，激活函数必须使用非线性函数。</p><ul><li>sigmoid函数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">\dfrac{1}{1 + e^{-x}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.09077em;vertical-align:-0.7693300000000001em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.697331em;"><span style="top:-2.989em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7693300000000001em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></li><li>阶跃函数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mo fence="true">{</mo><mtable rowspacing="0.3599999999999999em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>x</mi><mo>&lt;</mo><mo>=</mo><mn>0</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>x</mi><mo>&gt;</mo><mn>0</mn></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">f(x) = \begin{cases} 0 &amp; x &lt;= 0 \\ 1 &amp; x&gt;0 \end{cases}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.0000299999999998em;vertical-align:-1.25003em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size4">{</span></span><span class="mord"><span class="mtable"><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.69em;"><span style="top:-3.69em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord">0</span></span></span><span style="top:-2.25em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.19em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:1em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.69em;"><span style="top:-3.69em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&lt;</span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord">0</span></span></span><span style="top:-2.25em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.19em;"><span></span></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></li><li>ReLU(Rectified Linear Unit) 函数<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mo fence="true">{</mo><mtable rowspacing="0.3599999999999999em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>x</mi><mo>&lt;</mo><mo>=</mo><mn>0</mn></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mi>x</mi></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>x</mi><mo>&gt;</mo><mn>0</mn></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">f(x)=\begin{cases}0&amp;x&lt;=0\\x&amp;x&gt;0\end{cases}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.0000299999999998em;vertical-align:-1.25003em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size4">{</span></span><span class="mord"><span class="mtable"><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.69em;"><span style="top:-3.69em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord">0</span></span></span><span style="top:-2.25em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord mathnormal">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.19em;"><span></span></span></span></span></span><span class="arraycolsep" style="width:1em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.69em;"><span style="top:-3.69em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&lt;</span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord">0</span></span></span><span style="top:-2.25em;"><span class="pstrut" style="height:3.008em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.19em;"><span></span></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></li><li>Softmax 函数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><msup><mi>e</mi><msub><mi>x</mi><mi>i</mi></msub></msup><mrow><msubsup><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><msup><mi>e</mi><msub><mi>x</mi><mi>j</mi></msub></msup></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">y_i = \dfrac{e^{x_i}}{\sum\limits_{j=1}^{n} e^{x_j}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.6965660000000007em;vertical-align:-2.3551740000000008em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.341392em;"><span style="top:-2.1099999999999994em;"><span class="pstrut" style="height:3.3513970000000004em;"></span><span class="mord"><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3513970000000004em;"><span style="top:-2.122331em;margin-left:0em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.0000050000000003em;"><span class="pstrut" style="height:3em;"></span><span><span class="mop op-symbol small-op">∑</span></span></span><span style="top:-3.950005em;margin-left:0em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.113777em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6064620000000001em;"><span style="top:-3.0050700000000004em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2818857142857143em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.5813970000000004em;"><span class="pstrut" style="height:3.3513970000000004em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-4.028397em;"><span class="pstrut" style="height:3.3513970000000004em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.3551740000000008em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></li><li>Tanh函数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><msup><mi>e</mi><mi>x</mi></msup><mo>−</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow><mrow><msup><mi>e</mi><mi>x</mi></msup><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">f(x)=\dfrac{e^x-e^{-x}}{e^x+e^{-x}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.217661em;vertical-align:-0.7693300000000001em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.448331em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.590392em;"><span style="top:-2.9890000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.697331em;"><span style="top:-2.989em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.771331em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7693300000000001em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></li></ul><h3 id="损失函数"><a class="markdownIt-Anchor" href="#损失函数"></a> 损失函数</h3><ul><li>交叉熵（Cross Entropy）<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mi>N</mi></mfrac></mstyle><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><msubsup><mo>∑</mo><mrow><mi>c</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></msubsup><msub><mi>y</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>c</mi></mrow></msub><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>p</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>c</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\dfrac{1}{N} \sum\limits_{i=1}^N \sum\limits_{c=1}^C y_{i,c} \log(p_{i,c})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.506005em;vertical-align:-0.9776689999999999em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5283360000000004em;"><span style="top:-2.122331em;margin-left:0em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.0000050000000003em;"><span class="pstrut" style="height:3em;"></span><span><span class="mop op-symbol small-op">∑</span></span></span><span style="top:-3.950005em;margin-left:0em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9776689999999999em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5283360000000004em;"><span style="top:-2.132887em;margin-left:0em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">c</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.0000050000000003em;"><span class="pstrut" style="height:3em;"></span><span><span class="mop op-symbol small-op">∑</span></span></span><span style="top:-3.950005em;margin-left:0em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">C</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9671129999999999em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">c</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">c</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></li><li>均方误差 （Mean Squared Error）<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><mi>N</mi></mfrac></mstyle><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mover accent="true"><mi>y</mi><mo>^</mo></mover><mi>i</mi></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\dfrac{1}{N} \sum\limits_{i=1}^{N} (y_i - \hat{y}_i)^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.506005em;vertical-align:-0.9776689999999999em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5283360000000004em;"><span style="top:-2.122331em;margin-left:0em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.0000050000000003em;"><span class="pstrut" style="height:3em;"></span><span><span class="mop op-symbol small-op">∑</span></span></span><span style="top:-3.950005em;margin-left:0em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9776689999999999em;"><span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.19444em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></li></ul></blockquote><h2 id="神经网络的学习"><a class="markdownIt-Anchor" href="#神经网络的学习"></a> 神经网络的学习</h2><blockquote><h3 id="误差反向传播法"><a class="markdownIt-Anchor" href="#误差反向传播法"></a> 误差反向传播法</h3><ul><li>误差反向传播法（Backpropagation）是训练神经网络的核心算法，通过前向传播计算输出，再反向逐层计算损失函数对网络参数的梯度，利用链式法则高效传递误差信息，最后通过梯度下降法更新权重和偏置，逐步优化模型以最小化损失函数。</li></ul><h3 id="affine全连接层"><a class="markdownIt-Anchor" href="#affine全连接层"></a> Affine(全连接层)</h3><ul><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Y</mi><mo>=</mo><mi>X</mi><mo>⋅</mo><mi>W</mi><mo>+</mo><mi>B</mi></mrow><annotation encoding="application/x-tex">Y=X\cdot W+B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.05017em;">B</span></span></span></span></li><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>X</mi></mrow></mfrac></mstyle><mo>=</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>Y</mi></mrow></mfrac></mstyle><mo>⋅</mo><msup><mi>W</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">\dfrac{\partial L}{\partial X}=\dfrac{\partial L}{\partial Y}\cdot W^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.05744em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.37144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal">L</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.05744em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.37144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal">L</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8413309999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span></li><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>W</mi></mrow></mfrac></mstyle><mo>=</mo><msup><mi>X</mi><mi>T</mi></msup><mo>⋅</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>Y</mi></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">\dfrac{\partial L}{\partial W}=X^T \cdot\dfrac{\partial L}{\partial Y}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.05744em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.37144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal">L</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8413309999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:2.05744em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.37144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal">L</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></li><li>$\dfrac{\partial L}{\partial B}=\dfrac{\partial L}{\partial Y}  $ 的axis=<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span></span></span></span>方向上的和</li></ul><h3 id="softmax-with-loss"><a class="markdownIt-Anchor" href="#softmax-with-loss"></a> Softmax-with-Loss</h3><ul><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>x</mi><mi>i</mi></msub></mrow></mfrac></mstyle><mo>=</mo><msub><mi>y</mi><mi>i</mi></msub><mo>−</mo><msub><mi>t</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\dfrac{\partial L}{\partial x_i}=y_i-t_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.20744em;vertical-align:-0.8360000000000001em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.37144em;"><span style="top:-2.3139999999999996em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal">L</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8360000000000001em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.7777700000000001em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.76508em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></li></ul><h3 id="最优化"><a class="markdownIt-Anchor" href="#最优化"></a> 最优化</h3><ul><li>神经网络的学习的目的是找到使损失函数的值尽可能小的参数。这是寻找最优参数的问题，解决这个问题的过程称为最优化（optimization）。</li></ul><h4 id="sgdstochastic-gradient-descent"><a class="markdownIt-Anchor" href="#sgdstochastic-gradient-descent"></a> SGD(Stochastic Gradient Descent)</h4><ul><li>每次迭代只使用一个样本（或一个小批量）来计算梯度并沿梯度方向更新参数，并重复这个步骤多次，从而逐渐靠近最优参数。</li><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi><mo>=</mo><mi>W</mi><mo>−</mo><mi>η</mi><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>W</mi></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">W=W-\eta\dfrac{\partial L}{\partial W}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:2.05744em;vertical-align:-0.686em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">η</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.37144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal">L</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></li></ul><h4 id="momentum"><a class="markdownIt-Anchor" href="#momentum"></a> Momentum</h4><ul><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mo>=</mo><mi>α</mi><mi>v</mi><mo>−</mo><mi>η</mi><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>W</mi></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">v=\alpha v-\eta\dfrac{\partial L}{\partial W}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:2.05744em;vertical-align:-0.686em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">η</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.37144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal">L</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></li><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi><mo>=</mo><mi>W</mi><mo>+</mo><mi>v</mi></mrow><annotation encoding="application/x-tex">W=W+v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span></span></span></span></li></ul><h4 id="adagrad"><a class="markdownIt-Anchor" href="#adagrad"></a> AdaGrad</h4><ul><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mo>=</mo><mi>h</mi><mo>+</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>W</mi></mrow></mfrac></mstyle><mo>⊙</mo><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>W</mi></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">h=h+\dfrac{\partial L}{\partial W}\odot\dfrac{\partial L}{\partial W}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">h</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.77777em;vertical-align:-0.08333em;"></span><span class="mord mathnormal">h</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:2.05744em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.37144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal">L</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⊙</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:2.05744em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.37144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal">L</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></li><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi><mo>=</mo><mi>W</mi><mo>−</mo><mi>η</mi><mstyle displaystyle="true" scriptlevel="0"><mfrac><mn>1</mn><msqrt><mi>h</mi></msqrt></mfrac></mstyle><mstyle displaystyle="true" scriptlevel="0"><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>W</mi></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">W=W-\eta\dfrac{1}{\sqrt{h}}\dfrac{\partial L}{\partial W}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:2.30144em;vertical-align:-0.93em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">η</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.32144em;"><span style="top:-2.17778em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.93222em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord mathnormal">h</span></span></span><span style="top:-2.89222em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429c69,-144,104.5,-217.7,106.5,-221l0 -0c5.3,-9.3,12,-14,20,-14H400000v40H845.2724s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47zM834 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.10777999999999999em;"><span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.93em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.37144em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord mathnormal">L</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></li><li>AdaGrad会记录过去所有梯度的平方和。因此，学习越深入，更新的幅度就越小。实际上，如果无止境地学习，更新量就会变为0，完全不再更新。为了改善这个问题，可以使用RMSProp方法。RMSProp方法并不是将过去所有的梯度一视同仁地相加，而是逐渐地遗忘过去的梯度，在做加法运算时将新梯度的信息更多地反映出来。这种操作从专业上讲，称为“指数移动平均”，呈指数函数式地减小过去的梯度的尺度。</li></ul><h4 id="adamadaptive-moment-estimation"><a class="markdownIt-Anchor" href="#adamadaptive-moment-estimation"></a> Adam(Adaptive Moment Estimation)</h4><ul><li><strong>自适应学习率</strong>：<ul><li>Adam为每个参数单独调整学习率，基于梯度的一阶矩（均值）和二阶矩（未中心化的方差）的估计。</li><li>这使得Adam在处理稀疏梯度或噪声较大的问题时表现优异。</li></ul></li><li><strong>动量机制</strong>：<ul><li>Adam通过使用梯度的指数移动平均（类似于RMSProp）来引入动量，帮助加速收敛并逃离局部最优。</li></ul></li><li><strong>偏差校正</strong>：<ul><li>Adam在初始阶段对梯度的一阶矩和二阶矩进行偏差校正，避免初始估计偏差对训练的影响。</li></ul></li></ul><h3 id="权重初始化"><a class="markdownIt-Anchor" href="#权重初始化"></a> 权重初始化</h3><h4 id="xavier-初始化"><a class="markdownIt-Anchor" href="#xavier-初始化"></a> Xavier 初始化</h4><ul><li>适用于 Sigmoid 或 Tanh 激活函数。</li><li>权重从均匀分布或正态分布中采样，方差根据输入和输出的维度调整：<ul><li>均匀分布：<code>np.random.uniform(-sqrt(6/(fan_in + fan_out)), sqrt(6/(fan_in + fan_out)))</code></li><li>正态分布：<code>np.random.randn(shape) * sqrt(2/(fan_in + fan_out))</code></li></ul></li><li>其中，<code>fan_in</code> 是输入维度，<code>fan_out</code> 是输出维度。</li></ul><h4 id="he-初始化"><a class="markdownIt-Anchor" href="#he-初始化"></a> He 初始化</h4><ul><li>适用于 ReLU 及其变体（如 Leaky ReLU）激活函数。</li><li>权重从正态分布中采样，方差为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msqrt><mrow><mn>2</mn><mi mathvariant="normal">/</mi><mi>n</mi></mrow></msqrt></mrow><annotation encoding="application/x-tex">\sqrt{2/n}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.24em;vertical-align:-0.30499999999999994em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.935em;"><span class="svg-align" style="top:-3.2em;"><span class="pstrut" style="height:3.2em;"></span><span class="mord" style="padding-left:1em;"><span class="mord">2</span><span class="mord">/</span><span class="mord mathnormal">n</span></span></span><span style="top:-2.8950000000000005em;"><span class="pstrut" style="height:3.2em;"></span><span class="hide-tail" style="min-width:1.02em;height:1.28em;"><svg width='400em' height='1.28em' viewBox='0 0 400000 1296' preserveAspectRatio='xMinYMin slice'><path d='M263,681c0.7,0,18,39.7,52,119c34,79.3,68.167,158.7,102.5,238c34.3,79.3,51.8,119.3,52.5,120c340,-704.7,510.7,-1060.3,512,-1067l0 -0c4.7,-7.3,11,-11,19,-11H40000v40H1012.3s-271.3,567,-271.3,567c-38.7,80.7,-84,175,-136,283c-52,108,-89.167,185.3,-111.5,232c-22.3,46.7,-33.8,70.3,-34.5,71c-4.7,4.7,-12.3,7,-23,7s-12,-1,-12,-1s-109,-253,-109,-253c-72.7,-168,-109.3,-252,-110,-252c-10.7,8,-22,16.7,-34,26c-22,17.3,-33.3,26,-34,26s-26,-26,-26,-26s76,-59,76,-59s76,-60,76,-60zM1001 80h400000v40h-400000z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.30499999999999994em;"><span></span></span></span></span></span></span></span></span>，其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">n</span></span></span></span>是输入维度：<ul><li><code>np.random.randn(shape) * sqrt(2/fan_in)</code></li></ul></li></ul><h3 id="batch-normalization"><a class="markdownIt-Anchor" href="#batch-normalization"></a> Batch Normalization</h3><ul><li>以进行学习时的mini-batch为单位，按mini-batch进行使数据分布的均值为0、方差为1的正规化。</li><li>接着对正规化后的数据进行缩放和平移的变换</li></ul><h3 id="正则化"><a class="markdownIt-Anchor" href="#正则化"></a> 正则化</h3><ul><li>正则化（Regularization）是机器学习和统计学中的一种技术，用于防止模型过拟合（overfitting）。</li></ul><h4 id="权值衰减"><a class="markdownIt-Anchor" href="#权值衰减"></a> 权值衰减</h4><ul><li>权值衰减是一直以来经常被使用的一种抑制过拟合的方法。该方法通过在学习的过程中对大的权重进行惩罚，来抑制过拟合。很多过拟合原本就是因为权重参数取值过大才发生的。</li><li>比如为损失函数加上权重的平方范数（L2范数）。这样一来，就可以抑制权重变大。<br />用符号表示的话，如果将权重记为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span></span>，L2范数的权值衰减就是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mi>λ</mi><msup><mi>W</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\frac{1}{2}\lambda W^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord mathnormal">λ</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>，然后将这个值加到损失函数上。这里，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">λ</span></span></span></span>是控制正则化强度的超参数。此外，开头的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>是用于将 的求导结果变成λW的调整用常量。</li><li>对于所有权重，权值衰减方法都会为损失函数加上<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mi>λ</mi><msup><mi>W</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\frac{1}{2}\lambda W^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord mathnormal">λ</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span> 。因此，在求权重梯度的计算中，要为之前的误差反向传播法的结果加上正则化项的导数<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi><mi>W</mi></mrow><annotation encoding="application/x-tex">\lambda W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">λ</span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span></span></span></span>。</li></ul><h4 id="dropout"><a class="markdownIt-Anchor" href="#dropout"></a> Dropout</h4><ul><li>Dropout是一种在学习的过程中随机删除神经元的方法。训练时，随机选出隐藏层的神经元，然后将其删除。被删除的神经元不再进行信号的传递。训练时，每传递一次数据，就会随机选择要删除的神经元。测试时，虽然会传递所有的神经元信号，但是对于各个神经元的输出，要乘上训练时的删除比例后再输出。</li><li>集成学习与 Dropout有密切的关系。这是因为可以将 Dropout理解为，通过在学习过程中随机删除神经元，从而每一次都让不同的模型进行学习。并且，推理时，通过对神经元的输出乘以删除比例，可以取得模型的平均值。也就是说，可以理解成，Dropout将集成学习的效果（模拟地）通过一个网络实现了。</li></ul></blockquote><h2 id="卷积神经网络"><a class="markdownIt-Anchor" href="#卷积神经网络"></a> 卷积神经网络</h2><blockquote><h3 id="卷积层"><a class="markdownIt-Anchor" href="#卷积层"></a> 卷积层</h3><ul><li>卷积层的输入输出数据称为特征图（feature map）。其中，卷积层的输入数据称为输入特征图（input feature map），输出数据称为输出特征图（output feature map）。</li><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mi>C</mi><mo separator="true">,</mo><mi>H</mi><mo separator="true">,</mo><mi>W</mi><mo stretchy="false">)</mo><mo>∗</mo><mo stretchy="false">(</mo><mi>F</mi><mi>N</mi><mo separator="true">,</mo><mi>C</mi><mo separator="true">,</mo><mi>F</mi><mi>H</mi><mo separator="true">,</mo><mi>F</mi><mi>W</mi><mo stretchy="false">)</mo><mo>⇒</mo><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mi>F</mi><mi>N</mi><mo separator="true">,</mo><mi>O</mi><mi>H</mi><mo separator="true">,</mo><mi>O</mi><mi>W</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N,C, H, W)*(FN, C, FH, FW)\Rightarrow(N,FN, OH, OW)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">⇒</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mclose">)</span></span></span></span></li><li>数据 (batch_num*,* channel*,* height*,* width)。</li><li>滤波器的权重数据 (output_channel*,* input_channel*,* height*,* width)</li><li>偏置 (<em>FN,</em> 1*,* 1)</li></ul><h3 id="池化层"><a class="markdownIt-Anchor" href="#池化层"></a> 池化层</h3><ul><li>没有要学习的参数</li><li>通道数不发生变化</li><li>对微小的位置变化具有鲁棒性（健壮）</li><li>池化窗口<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>P</mi><mi>H</mi><mo separator="true">,</mo><mi>P</mi><mi>W</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(PH,PW)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mclose">)</span></span></span></span></li></ul><h3 id="经典cnn"><a class="markdownIt-Anchor" href="#经典cnn"></a> 经典CNN</h3><ul><li><strong>VGG</strong><ul><li><strong>小卷积核</strong>：VGG 网络主要使用 3x3 的卷积核，这种小尺寸的卷积核可以减少参数数量，同时通过堆叠多个卷积层来增加网络的深度和非线性表达能力。</li><li><strong>深度网络</strong>：VGG 网络有多个版本，最常见的是 VGG-16 和 VGG-19，分别包含 16 层和 19 层（包括卷积层和全连接层）。</li><li><strong>统一架构</strong>：VGG 的架构非常统一，每一层都使用相同的卷积核尺寸和步长，简化了网络设计。</li><li><strong>池化层</strong>：VGG 使用 2x2 的最大池化层来逐步减小特征图的尺寸。</li><li><strong>全连接层</strong>：在卷积层之后，VGG 使用了多个全连接层，最后通过 softmax 层进行分类。</li><li><strong>ReLU</strong>：除了最后一层全连接层和池化层，每一层卷积层和全连接层之后都会应用 ReLU 激活函数。</li></ul></li><li><strong>GoogLeNet</strong><ul><li><strong>Inception 模块</strong>：<ul><li>使用多尺度的卷积核（1x1、3x3、5x5）并行提取特征。</li><li>通过 1x1 卷积进行降维，减少计算量。</li></ul></li><li><strong>深度网络</strong>：<ul><li>GoogLeNet 包含 22 层（如果考虑 Inception 模块中的层，则更多）。</li></ul></li><li><strong>全局平均池化</strong>：<ul><li>使用全局平均池化替代全连接层，减少参数量。</li></ul></li><li><strong>辅助分类器</strong>：<ul><li>在网络中间层引入辅助分类器，缓解梯度消失问题。</li></ul></li></ul></li><li><strong>ResNet</strong><ul><li><strong>残差学习</strong>：<ul><li>引入了残差块（Residual Block），通过跳跃连接（Shortcut Connection）将输入直接传递到输出，使网络能够学习残差映射。</li></ul></li><li><strong>深度网络</strong>：<ul><li>ResNet 可以训练非常深的网络（如 ResNet-152 包含 152 层）。</li></ul></li><li><strong>缓解梯度消失</strong>：<ul><li>残差连接使得梯度能够直接回传到浅层，缓解了梯度消失问题。</li></ul></li></ul></li></ul></blockquote>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>聚类模型</title>
      <link href="/posts/5.html"/>
      <url>/posts/5.html</url>
      
        <content type="html"><![CDATA[<h1 id="聚类模型"><a class="markdownIt-Anchor" href="#聚类模型"></a> 聚类模型</h1><h2 id="k-means-聚类"><a class="markdownIt-Anchor" href="#k-means-聚类"></a> K-means 聚类</h2><blockquote><p>K-Means聚类是一种常用的无监督学习算法，用于将数据集划分为K个簇（cluster）。其目标是将相似的数据点分配到同一个簇中，同时使不同簇之间的数据点尽可能不同。</p><p>基本步骤</p><ul><li>初始化</li></ul><p>选择K个初始聚类中心（centroid），可以随机选择数据集中的K个点，或通过其他方法初始化。</p><ul><li>分配数据点</li></ul><p>对于每个数据点，计算其与K个聚类中心的距离（通常使用欧氏距离），并将其分配到距离最近的聚类中心所属的簇。</p><ul><li>更新聚类中心</li></ul><p>对于每个簇，重新计算其聚类中心，通常取该簇中所有数据点的均值。</p><ul><li>迭代</li></ul><p>重复步骤2和步骤3，直到聚类中心不再发生显著变化，或达到预定的迭代次数。</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成数据</span></span><br><span class="line">X, y = make_blobs(n_samples=<span class="number">300</span>, centers=<span class="number">4</span>, cluster_std=<span class="number">0.60</span>, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个包含两个子图的画布</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>, <span class="number">5</span>))  <span class="comment"># 设置画布大小</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 子图 1：原始数据</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)  <span class="comment"># 1 行 2 列，第 1 个子图</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], s=<span class="number">50</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Original Data&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置 K 值</span></span><br><span class="line">k = <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 KMeans 实例</span></span><br><span class="line">kmeans = KMeans(n_clusters=k)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拟合模型</span></span><br><span class="line">kmeans.fit(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取聚类结果</span></span><br><span class="line">labels = kmeans.predict(X)</span><br><span class="line">centroids = kmeans.cluster_centers_</span><br><span class="line"></span><br><span class="line"><span class="comment"># 子图 2：聚类结果</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)  <span class="comment"># 1 行 2 列，第 2 个子图</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=labels, s=<span class="number">50</span>, cmap=<span class="string">&#x27;viridis&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制聚类中心</span></span><br><span class="line">plt.scatter(centroids[:, <span class="number">0</span>], centroids[:, <span class="number">1</span>], c=<span class="string">&#x27;red&#x27;</span>, s=<span class="number">200</span>, alpha=<span class="number">0.75</span>, marker=<span class="string">&#x27;X&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;K-means Clustering&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示图形</span></span><br><span class="line">plt.tight_layout()  <span class="comment"># 自动调整子图间距</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="k-means-聚类-2"><a class="markdownIt-Anchor" href="#k-means-聚类-2"></a> K-means++ 聚类</h2><blockquote><p>传统的 K-means 算法随机选择初始质心，这可能导致算法收敛到局部最优解，或者需要更多的迭代次数才能收敛。K-means++ 通过一种更智能的方式选择初始质心，从而提高了算法的效率和结果的稳定性。</p><p>从数据集中随机选择一个点作为第一个质心。对于每一个非质心点，计算它与已选质心的最短距离 。根据概率分布选择下一个质心。距离越远的点被选中的概率越大。重复这一过程，直到选出 k 个质心。</p></blockquote><h2 id="k-medoids"><a class="markdownIt-Anchor" href="#k-medoids"></a> K-medoids</h2><blockquote><p>K-medoids 的目标是找到 k 个 medoids（簇中心），使得每个数据点到其所属簇的 medoid 的距离之和最小。与 K-means 不同，K-medoids 的 medoid 必须是数据集中的一个实际数据点。K-medoids 可以使用任意距离度量（如曼哈顿距离）。</p><p>基本步骤</p><ul><li>初始化：<ul><li>随机选择 k 个数据点作为初始 medoids。</li></ul></li><li>分配数据点到簇：<ul><li>对于每个数据点，将其分配到最近的 medoid 所在的簇。</li></ul></li><li>更新 medoids：<ul><li>对于每个簇，尝试用簇内的其他点替换当前的 medoid，计算替换后的总距离。</li><li>如果替换后的总距离更小，则更新 medoid。</li></ul></li><li>重复迭代：<ul><li>重复步骤 2 和 3，直到 medoids 不再变化或达到最大迭代次数。</li></ul></li></ul></blockquote><h2 id="层次聚类-hierarchical-clustering"><a class="markdownIt-Anchor" href="#层次聚类-hierarchical-clustering"></a> 层次聚类 (Hierarchical Clustering)</h2><blockquote><p>层次聚类是一种无监督学习方法，它不需要预先指定聚类的数量，而是生成一个由层次嵌套的聚类树（称为树状图或树形图）来表示数据的聚类结构。层次聚类主要有两种方式：凝聚型（Agglomerative）和分裂型（Divisive）。</p><ul><li><p>凝聚型层次聚类（Agglomerative Hierarchical Clustering）</p><ul><li>初始化：将每个数据点视为一个单独的聚类。</li><li>合并：计算每对聚类之间的距离，并合并距离最近的两个聚类。</li><li>重复：重复合并步骤，直到所有数据点都合并成一个大的聚类，或者达到某个特定的停止条件。</li><li>树状图：通过树状图展示聚类过程，树状图的每个节点代表一个聚类，节点的高度代表合并时的距离。</li></ul></li><li><p>分裂型层次聚类（Divisive Hierarchical Clustering）</p><ul><li>初始化：将所有数据点作为一个大的聚类开始。</li><li>分裂：选择一个聚类并将其分裂成两个子聚类。</li><li>重复：对每个子聚类重复分裂步骤，直到每个聚类只包含一个数据点，或者达到某个特定的停止条件。</li><li>树状图：通过树状图展示聚类过程，树状图的根节点代表初始的大聚类。</li></ul></li></ul></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> scipy.cluster.hierarchy <span class="keyword">import</span> dendrogram, linkage</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> AgglomerativeClustering</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成示例数据</span></span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line">X, _ = make_blobs(n_samples=<span class="number">50</span>, centers=<span class="number">3</span>, cluster_std=<span class="number">1.0</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用Scipy的linkage函数进行层次聚类</span></span><br><span class="line">linked = linkage(X, method=<span class="string">&#x27;ward&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制树状图</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">7</span>))</span><br><span class="line">dendrogram(linked, orientation=<span class="string">&#x27;top&#x27;</span>, distance_sort=<span class="string">&#x27;descending&#x27;</span>, show_leaf_counts=<span class="literal">True</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Hierarchical Clustering Dendrogram&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Sample index&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Distance&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用Scikit-learn的AgglomerativeClustering进行层次聚类</span></span><br><span class="line">cluster = AgglomerativeClustering(n_clusters=<span class="number">3</span>, affinity=<span class="string">&#x27;euclidean&#x27;</span>, linkage=<span class="string">&#x27;ward&#x27;</span>)</span><br><span class="line">cluster.fit_predict(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制聚类结果</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=cluster.labels_, cmap=<span class="string">&#x27;viridis&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Agglomerative Clustering&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Feature 1&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Feature 2&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="dbscan-聚类"><a class="markdownIt-Anchor" href="#dbscan-聚类"></a> DBSCAN  聚类</h2><blockquote><p>DBSCAN是一种基于密度的聚类算法，它能够识别出任意形状的簇，并且能够将噪声点识别出来。DBSCAN算法的主要思想是，如果一个区域内的密度（即点的数量）足够高，则该区域中的点可以被归为一个簇。DBSCAN算法不需要事先指定簇的数量，因此它在处理复杂形状和大小的簇时非常有用。</p><p>基本步骤</p><ul><li><p>初始化</p><ul><li>选择参数 <code>ε</code>（邻域半径）和 <code>min_samples</code>（最小点数）。</li><li>将所有点标记为未访问。</li></ul></li><li><p>遍历数据点</p><ul><li>对于每个未访问的点 p，检查其ε-邻域内的点数。</li><li>如果 p 是核心点，则创建一个新的簇，并将 p 及其密度可达的所有点加入该簇。</li><li>如果 p 是边界点或噪声点，则标记为噪声。</li></ul></li><li><p>重复</p><ul><li>重复上述过程，直到所有点都被访问。</li></ul></li></ul></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> DBSCAN</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_moons</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成示例数据（月牙形数据集）</span></span><br><span class="line">X, _ = make_moons(n_samples=<span class="number">300</span>, noise=<span class="number">0.05</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 DBSCAN 对象</span></span><br><span class="line">dbscan = DBSCAN(eps=<span class="number">0.3</span>, min_samples=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行聚类</span></span><br><span class="line">labels = dbscan.fit_predict(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出聚类结果</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Cluster Labels:&quot;</span>, labels)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化聚类结果</span></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">6</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制每个簇的点</span></span><br><span class="line">unique_labels = <span class="built_in">set</span>(labels)</span><br><span class="line">colors = [plt.cm.Spectral(each) <span class="keyword">for</span> each <span class="keyword">in</span> np.linspace(<span class="number">0</span>, <span class="number">1</span>, <span class="built_in">len</span>(unique_labels))]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> label, color <span class="keyword">in</span> <span class="built_in">zip</span>(unique_labels, colors):</span><br><span class="line">    <span class="keyword">if</span> label == -<span class="number">1</span>:</span><br><span class="line">        <span class="comment"># 噪声点用黑色表示</span></span><br><span class="line">        color = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">        cluster_name = <span class="string">&quot;Noise&quot;</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        cluster_name = <span class="string">f&quot;Cluster <span class="subst">&#123;label&#125;</span>&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 绘制属于当前簇的点</span></span><br><span class="line">    class_member_mask = (labels == label)</span><br><span class="line">    xy = X[class_member_mask]</span><br><span class="line">    plt.scatter(xy[:, <span class="number">0</span>], xy[:, <span class="number">1</span>], color=color, label=cluster_name, s=<span class="number">50</span>, edgecolors=<span class="string">&#x27;k&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">&#x27;DBSCAN Clustering&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Feature 1&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Feature 2&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="optics-聚类"><a class="markdownIt-Anchor" href="#optics-聚类"></a> OPTICS 聚类</h2><blockquote><p>OPTICS是一种基于密度的聚类算法，是DBSCAN算法的改进版本。与DBSCAN算法相比，OPTICS算法对输入参数（如eps和MinPts）的敏感度较低，因为它将eps参数从单个值放宽到一个范围。</p><p>核心思想是通过计算每个数据点的 核心距离和可达距离，生成一个有序的可达性图。通过分析可达性图，可以识别出不同密度的簇。</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> OPTICS</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成示例数据</span></span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line">X = np.random.rand(<span class="number">100</span>, <span class="number">2</span>)  <span class="comment"># 100 个二维数据点</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 OPTICS 聚类</span></span><br><span class="line">optics = OPTICS(min_samples=<span class="number">5</span>, xi=<span class="number">0.05</span>, min_cluster_size=<span class="number">0.1</span>)</span><br><span class="line">optics.fit(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取聚类结果</span></span><br><span class="line">labels = optics.labels_</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化聚类结果</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=labels, cmap=<span class="string">&#x27;viridis&#x27;</span>, s=<span class="number">50</span>, alpha=<span class="number">0.8</span>)</span><br><span class="line">plt.title(<span class="string">&quot;OPTICS Clustering&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="sting-聚类"><a class="markdownIt-Anchor" href="#sting-聚类"></a> STING 聚类</h2><blockquote><p>STING是一种基于网格的聚类算法，适用于处理大规模数据集。STING 将数据空间划分为多个网格单元，并通过统计信息（如均值、方差等）来描述每个单元中的数据分布。通过分层结构，STING 能够高效地处理数据，并且支持多分辨率聚类。<br />核心思想是将数据空间划分为多个网格单元，并构建一个分层结构（通常是四叉树或八叉树）。每个网格单元存储统计信息（如均值、方差、最小值、最大值等），通过这些信息可以快速判断单元是否属于某个簇。</p></blockquote><h2 id="clique-聚类"><a class="markdownIt-Anchor" href="#clique-聚类"></a> CLIQUE 聚类</h2><h2 id="gauss-混合模型"><a class="markdownIt-Anchor" href="#gauss-混合模型"></a> Gauss 混合模型</h2><h2 id="模糊-c-means"><a class="markdownIt-Anchor" href="#模糊-c-means"></a> 模糊 C-Means</h2>]]></content>
      
      
      <categories>
          
          <category> 数模 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>分类模型</title>
      <link href="/posts/3.html"/>
      <url>/posts/3.html</url>
      
        <content type="html"><![CDATA[<h1 id="分类模型"><a class="markdownIt-Anchor" href="#分类模型"></a> 分类模型</h1><blockquote><p>分类模型是数学建模中一种根据数据特征将数据集中的实例划分为不同类别或组的模型</p></blockquote><h2 id="逻辑回归logistic-regression"><a class="markdownIt-Anchor" href="#逻辑回归logistic-regression"></a> 逻辑回归（Logistic Regression）</h2><blockquote><p>逻辑回归是一种广义的线性回归分析模型，属于机器学习中的监督学习。其推导过程与计算方式类似于回归的过程，但实际上主要是用来解决二分类问题（也可以解决多分类问题）。逻辑回归通过拟合数据，预测某个事件发生的概率，并根据概率值进行分类。它的核心是通过 Sigmoid 函数将线性回归的输出映射到 [0, 1] 区间，表示概率值。</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score, classification_report</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据集（以二分类为例）</span></span><br><span class="line">data = load_iris()</span><br><span class="line">X = data.data[:<span class="number">100</span>, :]  <span class="comment"># 只取前两类</span></span><br><span class="line">y = data.target[:<span class="number">100</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分训练集和测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建逻辑回归模型,默认损失函数是对数损失</span></span><br><span class="line">model = LogisticRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 评估模型</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;准确率:&quot;</span>, accuracy_score(y_test, y_pred))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;分类报告:\n&quot;</span>, classification_report(y_test, y_pred))</span><br></pre></td></tr></table></figure><h2 id="决策树decision-tree"><a class="markdownIt-Anchor" href="#决策树decision-tree"></a> 决策树（Decision Tree）</h2><blockquote><p>决策树是一种常用的机器学习算法，广泛应用于分类和回归任务。它通过树状结构对数据进行分割，每个内部节点表示一个特征或属性的判断条件，每个分支代表一个可能的判断结果，而每个叶子节点则代表一个类别（分类任务）或一个值（回归任务）。</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 导入必要的库</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier, plot_tree</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载鸢尾花数据集</span></span><br><span class="line">data = load_iris()</span><br><span class="line">X = data.data  <span class="comment"># 特征</span></span><br><span class="line">y = data.target  <span class="comment"># 标签</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集划分：训练集和测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建决策树分类器</span></span><br><span class="line">model = DecisionTreeClassifier(max_depth=<span class="number">3</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设我们有一些无标签数据</span></span><br><span class="line">unlabeled_data = np.array([</span><br><span class="line">    [<span class="number">5.1</span>, <span class="number">3.5</span>, <span class="number">1.4</span>, <span class="number">0.2</span>],  <span class="comment"># 样本1</span></span><br><span class="line">    [<span class="number">6.7</span>, <span class="number">3.0</span>, <span class="number">5.2</span>, <span class="number">2.3</span>],  <span class="comment"># 样本2</span></span><br><span class="line">    [<span class="number">4.9</span>, <span class="number">3.1</span>, <span class="number">1.5</span>, <span class="number">0.1</span>],  <span class="comment"># 样本3</span></span><br><span class="line">    [<span class="number">5.8</span>, <span class="number">2.7</span>, <span class="number">4.1</span>, <span class="number">1.0</span>]   <span class="comment"># 样本4</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用训练好的模型对无标签数据进行预测</span></span><br><span class="line">predicted_labels = model.predict(unlabeled_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出预测结果</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Predicted Labels:&quot;</span>, predicted_labels)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果需要类别名称，可以映射到目标名称</span></span><br><span class="line">predicted_class_names = data.target_names[predicted_labels]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Predicted Class Names:&quot;</span>, predicted_class_names)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化决策树</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>, <span class="number">8</span>))</span><br><span class="line">plot_tree(model, filled=<span class="literal">True</span>, feature_names=data.feature_names, class_names=data.target_names)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="随机森林random-forest"><a class="markdownIt-Anchor" href="#随机森林random-forest"></a> 随机森林（Random Forest）</h2><blockquote><p>随机森林是一种基于集成学习的机器学习算法，广泛应用于分类和回归任务。它通过构建多个决策树并将它们的预测结果进行集成，从而提高模型的准确性和鲁棒性。</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line">data = load_iris()</span><br><span class="line">X, y = data.data, data.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分训练集和测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练随机森林模型</span></span><br><span class="line">model = RandomForestClassifier(n_estimators=<span class="number">100</span>, random_state=<span class="number">42</span>)</span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出预测结果</span></span><br><span class="line">packed_results = <span class="built_in">list</span>(<span class="built_in">zip</span>(y_test, y_pred))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;真实标签&amp;预测结果&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> true_label, pred_label <span class="keyword">in</span> packed_results:</span><br><span class="line">    <span class="built_in">print</span>(true_label,pred_label)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 评估模型</span></span><br><span class="line">accuracy = accuracy_score(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;模型准确率: <span class="subst">&#123;accuracy:<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><h2 id="朴素贝叶斯naive-bayes"><a class="markdownIt-Anchor" href="#朴素贝叶斯naive-bayes"></a> 朴素贝叶斯（Naive Bayes）</h2><blockquote><p>朴素贝叶斯（Naive Bayes）是一种基于贝叶斯定理的简单概率分类算法，它的“朴素”体现在假设特征之间相互独立，尽管现实中这一假设往往不成立，但朴素贝叶斯分类器仍然能够在许多实际应用中取得不错的效果。</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line">data = load_iris()</span><br><span class="line">X = data.data</span><br><span class="line">y = data.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分训练集和测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练朴素贝叶斯模型</span></span><br><span class="line">model = GaussianNB()</span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line"></span><br><span class="line">result = <span class="built_in">list</span>(<span class="built_in">zip</span>(y_test, y_pred))</span><br><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> result:</span><br><span class="line"> <span class="built_in">print</span>(x,y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算准确率</span></span><br><span class="line">accuracy = accuracy_score(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Accuracy: <span class="subst">&#123;accuracy:<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><h2 id="k近邻算法knn"><a class="markdownIt-Anchor" href="#k近邻算法knn"></a> K近邻算法（KNN）</h2><blockquote><p>K近邻算法是一种监督学习算法，主要用于分类和回归任务。它的核心思想是：给定一个样本，通过查找其最近的 K 个邻居，根据这些邻居的类别或值来预测该样本的类别或值。</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line">data = load_iris()</span><br><span class="line">X = data.data</span><br><span class="line">y = data.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分训练集和测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 KNN 模型（选择 K=3）</span></span><br><span class="line">knn = KNeighborsClassifier(n_neighbors=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">knn.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">y_pred = knn.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算准确率</span></span><br><span class="line">accuracy = accuracy_score(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;准确率: <span class="subst">&#123;accuracy:<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><h2 id="支持向量机svm"><a class="markdownIt-Anchor" href="#支持向量机svm"></a> 支持向量机（SVM）</h2><blockquote><p>支持向量机（Support Vector Machine, SVM） 是一种强大的监督学习算法，主要用于分类和回归任务。它的核心思想是找到一个最优的超平面，将不同类别的样本分开，并且最大化类别之间的边界（即“间隔”）。</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line">data = load_iris()</span><br><span class="line">X = data.data</span><br><span class="line">y = data.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分训练集和测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 SVM 模型（使用 RBF 核）</span></span><br><span class="line">svm = SVC(kernel=<span class="string">&#x27;rbf&#x27;</span>, C=<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">svm.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">y_pred = svm.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算准确率</span></span><br><span class="line">accuracy = accuracy_score(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;准确率: <span class="subst">&#123;accuracy:<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 数模 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>评价模型</title>
      <link href="/posts/2.html"/>
      <url>/posts/2.html</url>
      
        <content type="html"><![CDATA[<h1 id="评价模型"><a class="markdownIt-Anchor" href="#评价模型"></a> 评价模型</h1><h2 id="层次分析法ahp"><a class="markdownIt-Anchor" href="#层次分析法ahp"></a> 层次分析法（AHP）</h2><blockquote><p>层次分析法是一种将与决策有关的元素分解成目标、准则、方案等多个层次，并在此基础上进行定性和定量分析的决策方法。</p></blockquote><blockquote><p>基本步骤</p><ul><li><p>建立层次结构模型</p><p>将决策问题分解为目标、准则（或标准）和方案（或备选方案）等层次。</p></li><li><p>构造判断矩阵</p></li></ul><p>通过两两比较，确定各层次因素之间的相对重要性。</p><ul><li>计算权重</li></ul><p>通过数学方法（算数平均法、几何平均法和特征值法）计算各因素的权重。</p><ul><li>一致性检验</li></ul><p>检查判断矩阵的一致性，确保决策者的判断逻辑合理。</p><ul><li>综合决策</li></ul><p>根据权重和层次结构，计算各备选方案的总得分，从而做出决策。</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calculate_weights</span>(<span class="params">matrix</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    计算判断矩阵的权重。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 计算几何平均</span></span><br><span class="line">    geometric_mean = np.prod(matrix, axis=<span class="number">1</span>) ** (<span class="number">1</span> / matrix.shape[<span class="number">0</span>])</span><br><span class="line">    <span class="comment"># 归一化得到权重</span></span><br><span class="line">    weights = geometric_mean / np.<span class="built_in">sum</span>(geometric_mean)</span><br><span class="line">    <span class="keyword">return</span> weights</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">check_consistency</span>(<span class="params">matrix, weights</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    检查判断矩阵的一致性。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    n = matrix.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 计算加权和</span></span><br><span class="line">    weighted_sum = np.dot(matrix, weights)</span><br><span class="line">    <span class="comment"># 计算 lambda_max</span></span><br><span class="line">    lambda_max = np.mean(weighted_sum / weights)</span><br><span class="line">    <span class="comment"># 计算一致性指标 (CI)</span></span><br><span class="line">    CI = (lambda_max - n) / (n - <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 随机一致性指标 (RI)，根据 n 的值查表</span></span><br><span class="line">    RI = &#123;<span class="number">1</span>: <span class="number">0</span>, <span class="number">2</span>: <span class="number">0</span>, <span class="number">3</span>: <span class="number">0.58</span>, <span class="number">4</span>: <span class="number">0.90</span>, <span class="number">5</span>: <span class="number">1.12</span>, <span class="number">6</span>: <span class="number">1.24</span>, <span class="number">7</span>: <span class="number">1.32</span>, <span class="number">8</span>: <span class="number">1.41</span>, <span class="number">9</span>: <span class="number">1.45</span>, <span class="number">10</span>: <span class="number">1.49</span>&#125;</span><br><span class="line">    <span class="comment"># 计算一致性比率 (CR)</span></span><br><span class="line">    CR = CI / RI[n]</span><br><span class="line">    <span class="keyword">return</span> CR</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义准则判断矩阵</span></span><br><span class="line">criteria_matrix = np.array([</span><br><span class="line">    [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>],  <span class="comment"># 价格 vs 质量 vs 交货时间</span></span><br><span class="line">    [<span class="number">1</span>/<span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>],  <span class="comment"># 质量 vs 价格 vs 交货时间</span></span><br><span class="line">    [<span class="number">1</span>/<span class="number">5</span>, <span class="number">1</span>/<span class="number">2</span>, <span class="number">1</span>]  <span class="comment"># 交货时间 vs 价格 vs 质量</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算准则权重</span></span><br><span class="line">criteria_weights = calculate_weights(criteria_matrix)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;准则权重:&quot;</span>, criteria_weights)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查准则判断矩阵的一致性</span></span><br><span class="line">CR = check_consistency(criteria_matrix, criteria_weights)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;准则一致性比率 (CR):&quot;</span>, CR)</span><br><span class="line"><span class="keyword">if</span> CR &lt; <span class="number">0.1</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;准则判断矩阵一致性可接受。&quot;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;准则判断矩阵一致性不可接受，需要调整。&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义各准则下的供应商判断矩阵</span></span><br><span class="line">price_matrix = np.array([</span><br><span class="line">    [<span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>],  <span class="comment"># A vs B vs C</span></span><br><span class="line">    [<span class="number">1</span>/<span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>],  <span class="comment"># B vs A vs C</span></span><br><span class="line">    [<span class="number">1</span>/<span class="number">4</span>, <span class="number">1</span>/<span class="number">3</span>, <span class="number">1</span>]  <span class="comment"># C vs A vs B</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">quality_matrix = np.array([</span><br><span class="line">    [<span class="number">1</span>, <span class="number">1</span>/<span class="number">3</span>, <span class="number">1</span>/<span class="number">5</span>],  <span class="comment"># A vs B vs C</span></span><br><span class="line">    [<span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>/<span class="number">2</span>],  <span class="comment"># B vs A vs C</span></span><br><span class="line">    [<span class="number">5</span>, <span class="number">2</span>, <span class="number">1</span>]  <span class="comment"># C vs A vs B</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">delivery_matrix = np.array([</span><br><span class="line">    [<span class="number">1</span>, <span class="number">5</span>, <span class="number">3</span>],  <span class="comment"># A vs B vs C</span></span><br><span class="line">    [<span class="number">1</span>/<span class="number">5</span>, <span class="number">1</span>, <span class="number">1</span>/<span class="number">2</span>],  <span class="comment"># B vs A vs C</span></span><br><span class="line">    [<span class="number">1</span>/<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]  <span class="comment"># C vs A vs B</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算各准则下的供应商权重</span></span><br><span class="line">price_weights = calculate_weights(price_matrix)</span><br><span class="line">quality_weights = calculate_weights(quality_matrix)</span><br><span class="line">delivery_weights = calculate_weights(delivery_matrix)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;价格权重:&quot;</span>, price_weights)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;质量权重:&quot;</span>, quality_weights)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;交货时间权重:&quot;</span>, delivery_weights)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查各准则判断矩阵的一致性</span></span><br><span class="line">CR_price = check_consistency(price_matrix, price_weights)</span><br><span class="line">CR_quality = check_consistency(quality_matrix, quality_weights)</span><br><span class="line">CR_delivery = check_consistency(delivery_matrix, delivery_weights)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;价格判断矩阵一致性比率 (CR):&quot;</span>, CR_price)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;质量判断矩阵一致性比率 (CR):&quot;</span>, CR_quality)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;交货时间判断矩阵一致性比率 (CR):&quot;</span>, CR_delivery)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 综合得分</span></span><br><span class="line">supplier_scores = np.array([</span><br><span class="line">    np.<span class="built_in">sum</span>(price_weights * criteria_weights[<span class="number">0</span>]),</span><br><span class="line">    np.<span class="built_in">sum</span>(quality_weights * criteria_weights[<span class="number">1</span>]),</span><br><span class="line">    np.<span class="built_in">sum</span>(delivery_weights * criteria_weights[<span class="number">2</span>])</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出最终得分</span></span><br><span class="line">supplier_names = [<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;C&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> name, score <span class="keyword">in</span> <span class="built_in">zip</span>(supplier_names, supplier_scores):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;供应商 <span class="subst">&#123;name&#125;</span> 的综合得分: <span class="subst">&#123;score:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择最佳供应商</span></span><br><span class="line">best_supplier = supplier_names[np.argmax(supplier_scores)]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;最佳供应商是: <span class="subst">&#123;best_supplier&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><h2 id="主成分分析评价法pca"><a class="markdownIt-Anchor" href="#主成分分析评价法pca"></a> 主成分分析评价法(PCA)</h2><blockquote><p>主成分分析评价法是将多个变量通过线性变换以选出较少个数重要变量的一种多元统计分析方法。</p><p>主成分保留了原始变量绝大多数信息。各个主成分之间互不相关。每个主成分都是原始变量的线性组合。</p><p>使用主成分分析的前提条件是原始数据各个变量之间应有较强的线性相关关系。如果原始变量之间的线性相关程度很小, 它们之间不存在简化的数据结构, 这时进行主成分分析实际是没有意义的。所以, 应用主成分分析时, 首先要对其适用性进行统计检验。检验方法有巴特莱特球性检验，KMO检验等</p></blockquote><blockquote><p>根据权重和层次结构，计算各备选方案的总得分，从而做出决策。</p><p>基本步骤</p><ul><li><p>无量纲化</p></li><li><p>计算协方差矩阵</p></li><li><p>计算协方差矩阵的特征值和特征向量</p></li><li><p>选择主成分</p><p>根据特征值的大小，选择前 k 个最大的特征值对应的特征向量，作为主成分</p></li><li><p>构建投影矩阵</p><p>将选择的 k 个特征向量组成投影矩阵</p></li><li><p>数据降维</p><p>将原始数据 X 投影到主成分空间，得到降维后的数据</p></li></ul></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例数据</span></span><br><span class="line">X = np.random.rand(<span class="number">5</span>,<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化 PCA，设置主成分数量为 2</span></span><br><span class="line">pca = PCA(n_components=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拟合数据</span></span><br><span class="line">X_pca = pca.fit_transform(X)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n降维后的数据（主成分得分）：&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(X_pca)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看主成分 (n_components,n_features)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;主成分(pca.components_)：&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(pca.components_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看每个主成分的方差 (特征值)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;主成分的方差(pca.explained_variance_)：&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(pca.explained_variance_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看每个主成分的方差贡献率</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;主成分的方差贡献率(pca.explained_variance_ratio_)：&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(pca.explained_variance_ratio_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 主成分载荷 = 特征向量(主成分方向)与特征值(主成分的方差)的平方根相乘，形状为 (n_features,n_components)</span></span><br><span class="line">loadings = pca.components_.T * np.sqrt(pca.explained_variance_) <span class="comment">#广播变成 (n_features,n_components)  </span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;主成分载荷：&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(loadings)</span><br></pre></td></tr></table></figure><h2 id="模糊综合评价法"><a class="markdownIt-Anchor" href="#模糊综合评价法"></a> 模糊综合评价法</h2><blockquote><p>模糊综合评价法是一种基于模糊数学的综合评价方法，用于处理具有模糊性和不确定性的问题。它通过将定性指标转化为定量评价，适用于多因素、多层次的复杂系统评价。</p></blockquote><blockquote><p>基本步骤</p><ul><li>确定评价因素集<br />明确影响评价对象的各个因素，记为 U。</li><li>确定评语集<br />设定评价等级，记为 V。</li><li>确定权重集<br />为各因素分配权重，反映其重要性，记为W，且权重和为1。</li><li>构建模糊关系矩阵<br />通过专家打分或数据分析，确定每个因素对评语集的隶属度，形成模糊关系矩阵 R。</li><li>进行模糊合成运算<br />将权重集 W与模糊关系矩阵 R合成，得到模糊评价结果 B=W∘R。</li><li>得出综合评价结果<br />对 B 进行去模糊化处理，得到最终评价结果。</li></ul></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fuzzy_comprehensive_evaluation</span>(<span class="params">weights, fuzzy_relation_matrix, scores</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    模糊综合评价法</span></span><br><span class="line"><span class="string">    :param weights: 权重集，格式为 numpy 数组，例如 np.array([0.3, 0.5, 0.2])</span></span><br><span class="line"><span class="string">    :param fuzzy_relation_matrix: 模糊关系矩阵，格式为 numpy 二维数组</span></span><br><span class="line"><span class="string">    :param scores: 每个评语对应的分数，格式为 numpy 数组，例如 np.array([5, 4, 3, 2, 1])</span></span><br><span class="line"><span class="string">    :return: 模糊评价结果（隶属度）和系统总得分</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 检查权重和模糊关系矩阵的维度是否匹配</span></span><br><span class="line">    <span class="comment"># 在 NumPy 中，一维数组的形状表示为 (n,)，其中 n 是数组的长度</span></span><br><span class="line">    <span class="keyword">if</span> weights.shape[<span class="number">0</span>] != fuzzy_relation_matrix.shape[<span class="number">0</span>]:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;权重集和模糊关系矩阵的维度不匹配！&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 模糊合成运算（使用加权平均法）</span></span><br><span class="line">    evaluation_result = np.dot(weights, fuzzy_relation_matrix)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 归一化处理（确保评价结果的总和为1）</span></span><br><span class="line">    evaluation_result = evaluation_result / np.<span class="built_in">sum</span>(evaluation_result)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算系统总得分</span></span><br><span class="line">    total_score = np.dot(evaluation_result, scores)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> evaluation_result, total_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例数据</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 权重集：外观、性能、价格的权重分别为 0.3, 0.5, 0.2</span></span><br><span class="line">    weights = np.array([<span class="number">0.3</span>, <span class="number">0.5</span>, <span class="number">0.2</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 模糊关系矩阵：每一行对应一个因素，每一列对应一个评语</span></span><br><span class="line">    <span class="comment"># 例如，外观的模糊关系为 [0.6, 0.3, 0.1, 0.0, 0.0]</span></span><br><span class="line">    fuzzy_relation_matrix = np.array([</span><br><span class="line">        [<span class="number">0.6</span>, <span class="number">0.3</span>, <span class="number">0.1</span>, <span class="number">0.0</span>, <span class="number">0.0</span>],  <span class="comment"># 外观</span></span><br><span class="line">        [<span class="number">0.4</span>, <span class="number">0.4</span>, <span class="number">0.2</span>, <span class="number">0.0</span>, <span class="number">0.0</span>],  <span class="comment"># 性能</span></span><br><span class="line">        [<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.5</span>, <span class="number">0.2</span>, <span class="number">0.0</span>]   <span class="comment"># 价格</span></span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 每个评语对应的分数：好=5分，较好=4分，一般=3分，较差=2分，差=1分</span></span><br><span class="line">    scores = np.array([<span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 进行模糊综合评价</span></span><br><span class="line">    result, total_score = fuzzy_comprehensive_evaluation(weights, fuzzy_relation_matrix, scores)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出结果</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;模糊综合评价结果（隶属度）：&quot;</span>, result)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;系统总得分：&quot;</span>, total_score)</span><br></pre></td></tr></table></figure><h2 id="灰色综合评价法"><a class="markdownIt-Anchor" href="#灰色综合评价法"></a> 灰色综合评价法</h2><blockquote><p>灰色综合评价法是一种基于灰色系统理论的评价方法，适用于信息不完全或数据量较少的系统。通过灰色关联分析来确定各因素之间的关联度，从而进行综合评价。</p></blockquote><blockquote><p>基本步骤</p><ul><li><p>确定评价指标和样本数据</p></li><li><p>无量纲化</p><p>不推荐使用标准化</p></li><li><p>确定参考序列</p></li><li><p>计算灰色关联系数</p></li><li><p>计算灰色关联度</p></li><li><p>综合评价</p></li></ul></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">grey_relation_analysis</span>(<span class="params">data, reference_index=<span class="number">0</span>, rho=<span class="number">0.5</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    灰色关联度分析</span></span><br><span class="line"><span class="string">    :param data: 数据矩阵，形状为 (n, m)，n 为评价对象数，m 为评价指标数</span></span><br><span class="line"><span class="string">    :param reference_index: 参考序列的索引，默认为 0</span></span><br><span class="line"><span class="string">    :param rho: 分辨系数，默认为 0.5</span></span><br><span class="line"><span class="string">    :return: 灰色关联系数矩阵，形状为 (n, m)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 数据标准化处理（极差标准化）</span></span><br><span class="line">    normalized_data = (data - np.<span class="built_in">min</span>(data, axis=<span class="number">0</span>)) / (np.<span class="built_in">max</span>(data, axis=<span class="number">0</span>) - np.<span class="built_in">min</span>(data, axis=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 确定参考序列</span></span><br><span class="line">    reference_sequence = normalized_data[reference_index]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算差值矩阵</span></span><br><span class="line">    diff = np.<span class="built_in">abs</span>(normalized_data - reference_sequence)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算灰色关联系数</span></span><br><span class="line">    min_diff = np.<span class="built_in">min</span>(diff)</span><br><span class="line">    max_diff = np.<span class="built_in">max</span>(diff)</span><br><span class="line">    grey_relation_coefficients = (min_diff + rho * max_diff) / (diff + rho * max_diff)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> grey_relation_coefficients</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calculate_weights_grey_mean</span>(<span class="params">grey_relation_coefficients</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    计算指标权重（基于灰色关联度均值法，或可采用熵权法）</span></span><br><span class="line"><span class="string">    :param grey_relation_coefficients: 灰色关联系数矩阵，形状为 (n, m)</span></span><br><span class="line"><span class="string">    :return: 权重向量，形状为 (m,)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 计算每个指标的灰色关联度均值</span></span><br><span class="line">    mean_grey_relation = np.mean(grey_relation_coefficients, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算权重</span></span><br><span class="line">    weights = mean_grey_relation / np.<span class="built_in">sum</span>(mean_grey_relation)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> weights</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calculate_scores</span>(<span class="params">grey_relation_coefficients, weights</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    计算子序列综合得分</span></span><br><span class="line"><span class="string">    :param grey_relation_coefficients: 灰色关联系数矩阵，形状为 (n, m)</span></span><br><span class="line"><span class="string">    :param weights: 权重向量，形状为 (m,)</span></span><br><span class="line"><span class="string">    :return: 综合得分向量，形状为 (n,)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 计算综合得分</span></span><br><span class="line">    scores = np.dot(grey_relation_coefficients, weights)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> scores</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例数据</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 数据矩阵：4 个评价对象，3 个评价指标</span></span><br><span class="line">    data = np.array([</span><br><span class="line">        [<span class="number">80</span>, <span class="number">90</span>, <span class="number">70</span>],</span><br><span class="line">        [<span class="number">85</span>, <span class="number">88</span>, <span class="number">75</span>],</span><br><span class="line">        [<span class="number">78</span>, <span class="number">92</span>, <span class="number">72</span>],</span><br><span class="line">        [<span class="number">82</span>, <span class="number">89</span>, <span class="number">74</span>]</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 进行灰色关联度分析</span></span><br><span class="line">    grey_relation_coefficients = grey_relation_analysis(data)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算指标权重</span></span><br><span class="line">    weights = calculate_weights_grey_mean(grey_relation_coefficients)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算子序列综合得分</span></span><br><span class="line">    scores = calculate_scores(grey_relation_coefficients, weights)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出结果</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;灰色关联系数矩阵：\n&quot;</span>, grey_relation_coefficients)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;指标权重：&quot;</span>, weights)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;子序列综合得分：&quot;</span>, scores)</span><br></pre></td></tr></table></figure><h2 id="熵权法"><a class="markdownIt-Anchor" href="#熵权法"></a> 熵权法</h2><blockquote><p>熵权法是一种基于信息熵的客观赋权方法，用于确定各指标在综合评价中的权重。其核心思想是：指标的数据变异程度越大，提供的信息量越多，其权重也应越大。熵权法适用于多指标决策问题，能够减少主观赋权带来的偏差。</p></blockquote><blockquote><p>基本步骤</p><ul><li>无量纲化</li><li>计算第 i 个指标下第 j 个方案占总方案的比重</li><li>计算第 i 个指标的熵值</li><li>计算第 i 个指标的差异系数</li><li>计算第 i 个指标的权重</li><li>根据权重计算各方案的综合评价值</li></ul></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">entropy_weight</span>(<span class="params">data</span>):</span><br><span class="line">    <span class="comment"># 数据标准化</span></span><br><span class="line">    data_normalized = (data - np.<span class="built_in">min</span>(data, axis=<span class="number">0</span>)) / (np.<span class="built_in">max</span>(data, axis=<span class="number">0</span>) - np.<span class="built_in">min</span>(data, axis=<span class="number">0</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算比重</span></span><br><span class="line">    p = data_normalized / np.<span class="built_in">sum</span>(data_normalized, axis=<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算信息熵</span></span><br><span class="line">    k = <span class="number">1</span> / np.log(data.shape[<span class="number">0</span>])</span><br><span class="line">    e = -k * np.<span class="built_in">sum</span>(p * np.log(p + <span class="number">1e-10</span>), axis=<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算差异系数</span></span><br><span class="line">    d = <span class="number">1</span> - e</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算权重</span></span><br><span class="line">    w = d / np.<span class="built_in">sum</span>(d)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> w, data_normalized</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例数据</span></span><br><span class="line">data = np.array([[<span class="number">7</span>, <span class="number">9</span>, <span class="number">6</span>],</span><br><span class="line">                 [<span class="number">8</span>, <span class="number">7</span>, <span class="number">8</span>],</span><br><span class="line">                 [<span class="number">9</span>, <span class="number">6</span>, <span class="number">7</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算权重和标准化数据</span></span><br><span class="line">weights, data_normalized = entropy_weight(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算综合评价值</span></span><br><span class="line">scores = np.dot(data_normalized ,weights)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;权重:&quot;</span>, weights)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;综合评价值:&quot;</span>, scores)</span><br></pre></td></tr></table></figure><h2 id="bp神经网络评价法"><a class="markdownIt-Anchor" href="#bp神经网络评价法"></a> BP神经网络评价法</h2><blockquote><p>BP神经网络评价法是一种基于反向传播（Back Propagation, BP）神经网络的多指标综合评价方法。BP神经网络是一种多层前馈神经网络，通过反向传播算法调整网络参数，能够拟合复杂的非线性关系，适用于解决多指标、非线性的综合评价问题。</p></blockquote><blockquote><p>基本步骤</p><ul><li><p>确定评价指标和样本数据</p></li><li><p>无量纲化</p></li><li><p>构建神经网络模型</p><p>确定网络结构（如层数、每层的神经元数量）。</p><p>选择激活函数（如 Sigmoid、ReLU）和损失函数（如均方误差）。</p><p>初始化权重和偏置。</p></li><li><p>训练模型</p><p>使用训练集数据，通过反向传播算法调整权重和偏置，最小化损失函数。</p><p>设置训练参数（如学习率、迭代次数）。</p></li><li><p>模型验证</p><p>使用测试集数据验证模型的性能，评估预测精度。</p></li><li><p>综合评价</p><p>将待评价方案的指标数据输入训练好的模型，得到综合评价值。</p></li></ul></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例数据</span></span><br><span class="line">data = np.array([[<span class="number">7</span>, <span class="number">9</span>, <span class="number">6</span>],</span><br><span class="line">                 [<span class="number">8</span>, <span class="number">7</span>, <span class="number">8</span>],</span><br><span class="line">                 [<span class="number">9</span>, <span class="number">6</span>, <span class="number">7</span>],</span><br><span class="line">                 [<span class="number">6</span>, <span class="number">8</span>, <span class="number">7</span>],</span><br><span class="line">                 [<span class="number">7</span>, <span class="number">7</span>, <span class="number">9</span>]])</span><br><span class="line">target = np.array([<span class="number">0.8</span>, <span class="number">0.7</span>, <span class="number">0.6</span>, <span class="number">0.75</span>, <span class="number">0.85</span>])  <span class="comment"># 综合评价值</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据标准化</span></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">data_scaled = scaler.fit_transform(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分训练集和测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(data_scaled, target, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建神经网络模型</span></span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(Dense(<span class="number">10</span>, input_dim=<span class="number">3</span>, activation=<span class="string">&#x27;relu&#x27;</span>))  <span class="comment"># 隐藏层</span></span><br><span class="line">model.add(Dense(<span class="number">1</span>, activation=<span class="string">&#x27;linear&#x27;</span>))  <span class="comment"># 输出层</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 编译模型</span></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;adam&#x27;</span>, loss=<span class="string">&#x27;mean_squared_error&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">model.fit(X_train, y_train, epochs=<span class="number">100</span>, batch_size=<span class="number">2</span>, verbose=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型验证</span></span><br><span class="line">loss = model.evaluate(X_test, y_test, verbose=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;测试集损失:&quot;</span>, loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 综合评价</span></span><br><span class="line">new_data = np.array([[<span class="number">8</span>, <span class="number">8</span>, <span class="number">8</span>]])</span><br><span class="line">new_data_scaled = scaler.transform(new_data)</span><br><span class="line">predicted_score = model.predict(new_data_scaled)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;预测综合评价值:&quot;</span>, predicted_score)</span><br></pre></td></tr></table></figure><h2 id="数据包络分析法dea"><a class="markdownIt-Anchor" href="#数据包络分析法dea"></a> 数据包络分析法（DEA)</h2><blockquote><p>数据包络分析法是一种用于评估决策单元（Decision Making Units，DMUs）相对效率的非参数方法。它通过构建数学规划模型，比较具有相同输入和输出的DMUs的效率，从而识别出相对高效和低效的单位。</p></blockquote><blockquote><p>DEA主要有两种常用的模型：</p><p><strong>CCR模型</strong>(CRS)</p><ul><li>假设规模收益不变，即生产规模的扩大不会改变效率。</li><li>适用于评估在规模不变情况下的效率。</li></ul><p><strong>BCC模型</strong>(VRS)</p><ul><li>允许规模收益可变，即考虑规模经济或规模不经济的影响。</li><li>适用于评估在规模可变情况下的效率。</li></ul></blockquote><blockquote><p>基本步骤</p><ul><li><p>确定输入和输出指标</p></li><li><p>构建DEA模型</p><p>根据选择的模型（CCR或BCC），构建相应的线性规划模型。</p><p>模型的目标是最大化每个DMU的效率评分，同时确保其他DMUs的效率不超过1。</p></li><li><p>求解模型</p><p>使用线性规划求解器求解模型，得到每个DMU的效率评分。</p></li><li><p>分析结果</p><p>效率评分等于1的DMU被认为是在生产前沿面上，即相对高效。</p><p>效率评分小于1的DMU被认为是低效的，需要进行改进。</p></li></ul></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> skdeapackage <span class="keyword">import</span> dea</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入数据（资金, 人力）</span></span><br><span class="line">inputs = np.array([</span><br><span class="line">    [<span class="number">100</span>, <span class="number">50</span>],</span><br><span class="line">    [<span class="number">150</span>, <span class="number">70</span>],</span><br><span class="line">    [<span class="number">200</span>, <span class="number">80</span>],</span><br><span class="line">    [<span class="number">120</span>, <span class="number">60</span>]</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出数据（产量, 利润）</span></span><br><span class="line">outputs = np.array([</span><br><span class="line">    [<span class="number">80</span>, <span class="number">30</span>],</span><br><span class="line">    [<span class="number">100</span>, <span class="number">40</span>],</span><br><span class="line">    [<span class="number">120</span>, <span class="number">50</span>],</span><br><span class="line">    [<span class="number">90</span>, <span class="number">35</span>]</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义Min-Max归一化函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">min_max_normalize</span>(<span class="params">data</span>):</span><br><span class="line">    <span class="keyword">return</span> (data - np.<span class="built_in">min</span>(data)) / (np.<span class="built_in">max</span>(data) - np.<span class="built_in">min</span>(data))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 按列进行归一化</span></span><br><span class="line">inputs_normalized = np.apply_along_axis(min_max_normalize, <span class="number">0</span>, inputs)</span><br><span class="line">outputs_normalized = np.apply_along_axis(min_max_normalize, <span class="number">0</span>, outputs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算效率分数，选择CRS模型</span></span><br><span class="line">efficiency_scores = dea(inputs_normalized, outputs_normalized, model=<span class="string">&#x27;crs&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出效率分数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;效率分数:&quot;</span>, efficiency_scores)</span><br></pre></td></tr></table></figure><h2 id="优劣解距离法topsis"><a class="markdownIt-Anchor" href="#优劣解距离法topsis"></a> 优劣解距离法(TOPSIS)</h2><blockquote><p>基于各方案与理想解和负理想解的距离来评价方案的优劣。距离理想解越近且距离负理想解越远的方案，被认为是最优方案。</p></blockquote><blockquote><p>基本步骤</p><ul><li><p>构建决策矩阵<br />将备选方案和评价指标以矩阵形式表示，其中行代表方案，列代表评价指标。</p></li><li><p>无量纲化决策矩阵</p></li><li><p>确定权重<br />根据各指标的重要性，为每个指标赋予权重。权重可以通过主观方法（如专家打分）或客观方法（如熵值法）确定。</p></li><li><p>计算加权标准化矩阵<br />将标准化后的矩阵与权重相乘，得到加权标准化矩阵。</p></li><li><p>确定理想解和负理想解</p><p>理想解：每个指标的最优值（正向指标取最大值，负向指标取最小值）。</p><p>负理想解：每个指标的最劣值（正向指标取最小值，负向指标取最大值）。</p></li><li><p>计算距离<br />计算每个方案与理想解和负理想解的欧氏距离。</p></li><li><p>计算相对接近度<br />通过公式计算每个方案与理想解的相对接近度，值越大表示方案越优。</p></li><li><p>排序和选择<br />根据相对接近度对方案进行排序，选择最优方案。</p></li></ul></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">topsis</span>(<span class="params">decision_matrix, weights, impacts</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    TOPSIS算法实现</span></span><br><span class="line"><span class="string">    :param decision_matrix: 决策矩阵，每一行代表一个方案，每一列代表一个指标</span></span><br><span class="line"><span class="string">    :param weights: 各指标的权重</span></span><br><span class="line"><span class="string">    :param impacts: 各指标的影响方向（+1表示正向指标，-1表示负向指标）</span></span><br><span class="line"><span class="string">    :return: 相对接近度及排序结果</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 1. 标准化决策矩阵</span></span><br><span class="line">    normalized_matrix = decision_matrix / np.sqrt((decision_matrix ** <span class="number">2</span>).<span class="built_in">sum</span>(axis=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. 计算加权标准化矩阵</span></span><br><span class="line">    weighted_matrix = normalized_matrix * weights</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3. 确定理想解和负理想解</span></span><br><span class="line">    ideal_best = np.<span class="built_in">max</span>(weighted_matrix * impacts, axis=<span class="number">0</span>)</span><br><span class="line">    ideal_worst = np.<span class="built_in">min</span>(weighted_matrix * impacts, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4. 计算距离</span></span><br><span class="line">    distance_best = np.sqrt(((weighted_matrix - ideal_best) ** <span class="number">2</span>).<span class="built_in">sum</span>(axis=<span class="number">1</span>))</span><br><span class="line">    distance_worst = np.sqrt(((weighted_matrix - ideal_worst) ** <span class="number">2</span>).<span class="built_in">sum</span>(axis=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5. 计算相对接近度</span></span><br><span class="line">    closeness = distance_worst / (distance_best + distance_worst)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 6. 排序</span></span><br><span class="line">    ranked_indices = np.argsort(closeness)[::-<span class="number">1</span>]  <span class="comment"># 从大到小排序</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> closeness, ranked_indices</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例数据</span></span><br><span class="line">decision_matrix = np.array([</span><br><span class="line">    [<span class="number">7</span>, <span class="number">9</span>, <span class="number">9</span>, <span class="number">8</span>],  <span class="comment"># 方案1</span></span><br><span class="line">    [<span class="number">8</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">7</span>],  <span class="comment"># 方案2</span></span><br><span class="line">    [<span class="number">9</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">9</span>],  <span class="comment"># 方案3</span></span><br><span class="line">    [<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">6</span>]   <span class="comment"># 方案4</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">weights = np.array([<span class="number">0.3</span>, <span class="number">0.2</span>, <span class="number">0.3</span>, <span class="number">0.2</span>])  <span class="comment"># 各指标权重</span></span><br><span class="line">impacts = np.array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])         <span class="comment"># 各指标影响方向（1表示正向，-1表示负向）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行TOPSIS</span></span><br><span class="line">closeness, ranked_indices = topsis(decision_matrix, weights, impacts)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;相对接近度:&quot;</span>, closeness)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;方案排序:&quot;</span>, ranked_indices + <span class="number">1</span>)  <span class="comment"># 方案编号从1开始</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 数模 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>预测模型</title>
      <link href="/posts/4.html"/>
      <url>/posts/4.html</url>
      
        <content type="html"><![CDATA[<h1 id="预测模型"><a class="markdownIt-Anchor" href="#预测模型"></a> 预测模型</h1><h2 id="神经网络预测"><a class="markdownIt-Anchor" href="#神经网络预测"></a> 神经网络预测</h2><blockquote><p>神经网络预测是指利用神经网络模型对未知数据进行预测或分类的过程。</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Dense</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成示例数据</span></span><br><span class="line">X = np.random.rand(<span class="number">1000</span>, <span class="number">10</span>)</span><br><span class="line">y = np.<span class="built_in">sum</span>(X, axis=<span class="number">1</span>) + np.random.normal(<span class="number">0</span>, <span class="number">0.1</span>, <span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据分割</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据标准化</span></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">X_train = scaler.fit_transform(X_train)</span><br><span class="line">X_test = scaler.transform(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建模型</span></span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(Dense(<span class="number">64</span>, input_dim=<span class="number">10</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model.add(Dense(<span class="number">32</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model.add(Dense(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 编译模型</span></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;adam&#x27;</span>, loss=<span class="string">&#x27;mse&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">model.fit(X_train, y_train, epochs=<span class="number">50</span>, batch_size=<span class="number">32</span>, validation_split=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测</span></span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 评估</span></span><br><span class="line">mse = np.mean((y_test - y_pred.flatten()) ** <span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Mean Squared Error: <span class="subst">&#123;mse&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><h2 id="灰色预测"><a class="markdownIt-Anchor" href="#灰色预测"></a> 灰色预测</h2><blockquote><p>灰色预测是一种对含有不确定因素的时间序列进行预测的方法，特别适用于数据量少且数据波动较大的情况。其基本思想是通过生成灰色序列，对原始数据进行处理，提高数据的规律性，从而建立预测模型进行预测。</p></blockquote><h2 id="线性回归linear-regression"><a class="markdownIt-Anchor" href="#线性回归linear-regression"></a> 线性回归（Linear Regression）</h2><blockquote><p>通过拟合自变量与因变量之间的线性关系来预测目标值。目标是找到一条直线，使得预测值与实际值的误差最小（通常使用最小二乘法）。</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error, r2_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成随机数据</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">X = np.random.rand(<span class="number">100</span>, <span class="number">1</span>) * <span class="number">10</span>  <span class="comment"># 100个样本，1个特征</span></span><br><span class="line"><span class="comment"># 创建一个线性关系，添加一些噪声</span></span><br><span class="line">y = <span class="number">3</span> * X.squeeze() + <span class="number">2</span> + np.random.randn(<span class="number">100</span>) * <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分训练集和测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建线性回归模型</span></span><br><span class="line">model = LinearRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用测试集进行预测</span></span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算均方误差</span></span><br><span class="line">mse = mean_squared_error(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;均方误差: <span class="subst">&#123;mse:<span class="number">.2</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算R²分数</span></span><br><span class="line">r2 = r2_score(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;R²分数: <span class="subst">&#123;r2:<span class="number">.2</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="时间序列预测"><a class="markdownIt-Anchor" href="#时间序列预测"></a> 时间序列预测</h2><blockquote><p>时间序列预测是一种基于历史数据来预测未来值的技术。</p><p>单变量时间序列预测：只依赖于一个单一的时间序列数据源。单变量预测方法包括ARIMA（自回归积分滑动平均模型），SARIMA（季节性自回归积分滑动平均模型），指数平滑法，自回归（AR）模型，移动平均（MA）模型。</p><p>多变量时间序列预测：使用两个或更多的相关时间序列来进行预测。多变量预测方法包括向量自回归（VAR）模型，状态空间模型，SARIMAX（季节性自回归积分滑动平均模型扩展），机器学习方法：如随机森林、梯度提升机（GBM）、支持向量机（SVM）等，深度学习方法：如循环神经网络（RNN）、长短期记忆网络（LSTM）。</p></blockquote><h2 id="lstm"><a class="markdownIt-Anchor" href="#lstm"></a> LSTM</h2><blockquote></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> LSTM, Dense, Dropout</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler, MinMaxScaler</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例数据</span></span><br><span class="line">date_range = pd.date_range(start=<span class="string">&#x27;2023-01-01&#x27;</span>, periods=<span class="number">1000</span>, freq=<span class="string">&#x27;D&#x27;</span>)</span><br><span class="line">y = np.sin(np.linspace(<span class="number">0</span>, <span class="number">100</span>, <span class="number">1000</span>)) + np.random.normal(<span class="number">0</span>, <span class="number">0.1</span>, <span class="number">1000</span>)  <span class="comment"># 实际值</span></span><br><span class="line">X = pd.DataFrame(&#123;</span><br><span class="line">    <span class="string">&#x27;date&#x27;</span>: date_range,</span><br><span class="line">    <span class="string">&#x27;day&#x27;</span>: date_range.day,</span><br><span class="line">    <span class="string">&#x27;month&#x27;</span>: date_range.month,</span><br><span class="line">    <span class="string">&#x27;year&#x27;</span>: date_range.year,</span><br><span class="line">    <span class="string">&#x27;dayofweek&#x27;</span>: date_range.dayofweek,</span><br><span class="line">    <span class="string">&#x27;lag_1&#x27;</span>: np.roll(y, <span class="number">1</span>),  <span class="comment"># 前一天的值</span></span><br><span class="line">    <span class="string">&#x27;lag_2&#x27;</span>: np.roll(y, <span class="number">2</span>)   <span class="comment"># 前两天的值\</span></span><br><span class="line">    <span class="comment"># 使用lag_1和lag_2引入前一天和前两天的值作为模型的输入特征，有助于捕捉时间序列的     # 趋势和周期变化。大大提高预测准确性</span></span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除前两行</span></span><br><span class="line">X = X[<span class="number">2</span>:]</span><br><span class="line">y = y[<span class="number">2</span>:]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 numpy 将 X 和 y 转换为适合 LSTM 的三维张量格式</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_dataset</span>(<span class="params">X, y, time_steps=<span class="number">1</span></span>):</span><br><span class="line">    Xs, ys = [], []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(X) - time_steps):</span><br><span class="line">        Xs.append(X[i:(i + time_steps)])</span><br><span class="line">        ys.append(y[i + time_steps])</span><br><span class="line">    <span class="keyword">return</span> np.array(Xs), np.array(ys)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置时间步长</span></span><br><span class="line">time_steps = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 标准化特征, 对结果影响非常大!!!</span></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">X = scaler.fit_transform(X[[<span class="string">&#x27;day&#x27;</span>, <span class="string">&#x27;month&#x27;</span>, <span class="string">&#x27;year&#x27;</span>, <span class="string">&#x27;dayofweek&#x27;</span>, <span class="string">&#x27;lag_1&#x27;</span>, <span class="string">&#x27;lag_2&#x27;</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据转换为 LSTM 输入格式</span></span><br><span class="line">X, y = create_dataset(X, y, time_steps)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 划分训练集和测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建LSTM模型</span></span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(LSTM(<span class="number">50</span>, return_sequences=<span class="literal">True</span>, input_shape=(X_train.shape[<span class="number">1</span>], X_train.shape[<span class="number">2</span>])))</span><br><span class="line">model.add(Dropout(<span class="number">0.2</span>))</span><br><span class="line">model.add(LSTM(<span class="number">50</span>, return_sequences=<span class="literal">False</span>))</span><br><span class="line">model.add(Dropout(<span class="number">0.2</span>))</span><br><span class="line">model.add(Dense(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;adam&#x27;</span>, loss=<span class="string">&#x27;mean_squared_error&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">model.fit(X_train, y_train, epochs=<span class="number">10</span>, batch_size=<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制预测值和实际值</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>, <span class="number">6</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制实际值</span></span><br><span class="line">plt.plot(date_range[-<span class="built_in">len</span>(y_test):], y_test, label=<span class="string">&#x27;Actual&#x27;</span>, color=<span class="string">&#x27;blue&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制预测值</span></span><br><span class="line">plt.plot(date_range[-<span class="built_in">len</span>(y_test):], y_pred, label=<span class="string">&#x27;Predicted&#x27;</span>, color=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加标题和标签</span></span><br><span class="line">plt.title(<span class="string">&#x27;Actual vs Predicted&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Date&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Value&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示图表</span></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算均方误差</span></span><br><span class="line">mse = mean_squared_error(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Mean Squared Error: <span class="subst">&#123;mse&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="arima"><a class="markdownIt-Anchor" href="#arima"></a> ARIMA</h2><blockquote><p>ARIMA是一种经典的时间序列预测模型，它结合了自回归（AR）、差分（I）和移动平均（MA）三个部分，适用于处理非平稳时间序列数据。</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> statsmodels.tsa.arima.model <span class="keyword">import</span> ARIMA</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例数据：月度销售数据</span></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;Month&#x27;</span>: [<span class="string">&#x27;2020-01&#x27;</span>, <span class="string">&#x27;2020-02&#x27;</span>, <span class="string">&#x27;2020-03&#x27;</span>, <span class="string">&#x27;2020-04&#x27;</span>, <span class="string">&#x27;2020-05&#x27;</span>,</span><br><span class="line">              <span class="string">&#x27;2020-06&#x27;</span>, <span class="string">&#x27;2020-07&#x27;</span>, <span class="string">&#x27;2020-08&#x27;</span>, <span class="string">&#x27;2020-09&#x27;</span>, <span class="string">&#x27;2020-10&#x27;</span>,</span><br><span class="line">              <span class="string">&#x27;2020-11&#x27;</span>, <span class="string">&#x27;2020-12&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;Sales&#x27;</span>: [<span class="number">200</span>, <span class="number">220</span>, <span class="number">250</span>, <span class="number">230</span>, <span class="number">300</span>, <span class="number">280</span>, <span class="number">310</span>, <span class="number">320</span>, <span class="number">330</span>, <span class="number">360</span>, <span class="number">400</span>, <span class="number">450</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 DataFrame</span></span><br><span class="line">df = pd.DataFrame(data)</span><br><span class="line"></span><br><span class="line">df[<span class="string">&#x27;Month&#x27;</span>] = pd.to_datetime(df[<span class="string">&#x27;Month&#x27;</span>])</span><br><span class="line">df.set_index(<span class="string">&#x27;Month&#x27;</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建和拟合 ARIMA 模型 (p=12, d=1, q=12)</span></span><br><span class="line">model = ARIMA(df[<span class="string">&#x27;Sales&#x27;</span>], order=(<span class="number">12</span>, <span class="number">1</span>, <span class="number">12</span>))</span><br><span class="line">model_fit = model.fit()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行预测</span></span><br><span class="line">forecast = model_fit.forecast(steps=<span class="number">6</span>)  <span class="comment"># 预测未来 6 个月</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Forecasted values:&#x27;</span>, forecast)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制预测结果</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>] = [<span class="string">&#x27;SimHei&#x27;</span>]  <span class="comment"># 使用黑体</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="literal">False</span>  <span class="comment"># 解决负号显示问题</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">4</span>))</span><br><span class="line">plt.plot(df[<span class="string">&#x27;Sales&#x27;</span>], label=<span class="string">&#x27;原始销量&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>)</span><br><span class="line">plt.plot(pd.date_range(start=<span class="string">&#x27;2020-12-31&#x27;</span>, periods=<span class="number">6</span>, freq=<span class="string">&#x27;M&#x27;</span>), forecast, label=<span class="string">&#x27;预测&#x27;</span>, color=<span class="string">&#x27;red&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;ARIMA 预测&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;月份&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;销量&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="梯度提升树gbdt"><a class="markdownIt-Anchor" href="#梯度提升树gbdt"></a> 梯度提升树（GBDT）</h2><blockquote><p>梯度提升树是一种基于集成学习的算法，通过组合多个弱学习器来构建一个强学习器，其核心思想是逐步优化模型，每一步都训练一个新的决策树来拟合前一步的残差，从而不断改进模型的预测能力。</p><p>网格搜索是一种用于模型超参数调优的方法，它通过穷举搜索指定参数空间的所有可能组合，找出最佳的参数配置。</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_regression</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split, GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成合成数据</span></span><br><span class="line">X, y = make_regression(n_samples=<span class="number">1000</span>, n_features=<span class="number">10</span>, noise=<span class="number">0.1</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据分为训练集和测试集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化 GBDT 回归器</span></span><br><span class="line">gbdt = GradientBoostingRegressor(random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义超参数网格</span></span><br><span class="line">param_grid = &#123;</span><br><span class="line">    <span class="string">&#x27;n_estimators&#x27;</span>: [<span class="number">50</span>, <span class="number">100</span>, <span class="number">200</span>],  <span class="comment"># 树的数量</span></span><br><span class="line">    <span class="string">&#x27;learning_rate&#x27;</span>: [<span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">0.2</span>],  <span class="comment"># 学习率</span></span><br><span class="line">    <span class="string">&#x27;max_depth&#x27;</span>: [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>],  <span class="comment"># 树的最大深度</span></span><br><span class="line">    <span class="string">&#x27;min_samples_split&#x27;</span>: [<span class="number">2</span>, <span class="number">5</span>, <span class="number">10</span>],  <span class="comment"># 分裂节点所需的最小样本数</span></span><br><span class="line">    <span class="string">&#x27;min_samples_leaf&#x27;</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>],  <span class="comment"># 叶节点所需的最小样本数</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 GridSearchCV 进行超参数搜索</span></span><br><span class="line">grid_search = GridSearchCV(</span><br><span class="line">    estimator=gbdt,</span><br><span class="line">    param_grid=param_grid,</span><br><span class="line">    scoring=<span class="string">&#x27;neg_mean_squared_error&#x27;</span>,  <span class="comment"># 使用负均方误差作为评分标准</span></span><br><span class="line">    cv=<span class="number">5</span>,  <span class="comment"># 5 折交叉验证</span></span><br><span class="line">    n_jobs=-<span class="number">1</span>,  <span class="comment"># 使用所有可用的 CPU 核心</span></span><br><span class="line">    verbose=<span class="number">2</span>  <span class="comment"># 输出详细信息</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在训练集上拟合模型</span></span><br><span class="line">grid_search.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出最优参数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;最优参数:&quot;</span>, grid_search.best_params_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用最优参数的模型进行预测</span></span><br><span class="line">best_gbdt = grid_search.best_estimator_</span><br><span class="line">y_pred = best_gbdt.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算测试集上的均方误差</span></span><br><span class="line">mse = mean_squared_error(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;测试集上的均方误差 (MSE): <span class="subst">&#123;mse:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 数模 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>优化模型-规划模型</title>
      <link href="/posts/1.html"/>
      <url>/posts/1.html</url>
      
        <content type="html"><![CDATA[<h1 id="优化模型-规划模型"><a class="markdownIt-Anchor" href="#优化模型-规划模型"></a> 优化模型-规划模型</h1><h2 id="数学规划"><a class="markdownIt-Anchor" href="#数学规划"></a> 数学规划</h2><blockquote><p>数学规划（Mathematical Programming）是运筹学的一个分支，研究在给定的条件（约束条件）下，如何按照某一衡量指标（目标函数）来寻求计划、管理、工作中的最优方案。通俗来讲就是：求目标函数在一定约束条件下的极值问题。</p></blockquote><h2 id="线性规划lp"><a class="markdownIt-Anchor" href="#线性规划lp"></a> 线性规划(LP)</h2><blockquote><p>定义：目标函数和约束条件均是决策变量的线性表达式。</p><p>线性规划有通用求准确解的方法，它的最优解只存在于可行域的边界上。</p><p>中小规模问题：使用单纯形法。</p><p>大规模问题：使用内点法。</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> optimize <span class="keyword">as</span> op</span><br><span class="line"></span><br><span class="line"><span class="comment"># 给出变量取值范围</span></span><br><span class="line">x1 = (<span class="number">0</span>, <span class="literal">None</span>)</span><br><span class="line">x2 = (<span class="number">0</span>, <span class="literal">None</span>)</span><br><span class="line">x3 = (<span class="number">0</span>, <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">c = np.array([-<span class="number">2</span>, -<span class="number">3</span>, <span class="number">5</span>])  <span class="comment"># 目标函数系数,3x1列向量</span></span><br><span class="line"></span><br><span class="line">A_ub = np.array([[-<span class="number">2</span>, <span class="number">5</span>, -<span class="number">1</span>], [<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>]])  <span class="comment"># 不等式约束系数A，2x3维矩阵</span></span><br><span class="line">B_ub = np.array([-<span class="number">10</span>, <span class="number">12</span>])  <span class="comment"># 等式约束系数b, 2x1维列向量</span></span><br><span class="line">A_eq = np.array([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]])  <span class="comment"># 等式约束系数Aeq，3x1维列向量</span></span><br><span class="line">B_eq = np.array([<span class="number">7</span>])  <span class="comment"># 等式约束系数beq，1x1数值</span></span><br><span class="line"></span><br><span class="line">res = op.linprog(c, A_ub, B_ub, A_eq, B_eq, bounds=(x1, x2, x3), method=<span class="string">&#x27;highs&#x27;</span>)  <span class="comment"># 调用函数进行求解</span></span><br><span class="line"><span class="comment">#奇怪的警告,scipy还是牢版本,不管了</span></span><br></pre></td></tr></table></figure><h2 id="非线性规划nlp"><a class="markdownIt-Anchor" href="#非线性规划nlp"></a> 非线性规划(NLP)</h2><blockquote><p>定义：目标函数或约束条件中存在非线性表达式。</p><p>非线性规划的最优解（若存在）可能在其可行域的任一点达到，目前非线性规划还没有适合各种问题的一般解法，各种方法都有其特定的适用范围。</p></blockquote><blockquote><p>Scipy 工具包中的 optimize 模块可求解常见的非线性规划问题：</p><p>brent()：SciPy.optimize 模块中求解<strong>单变量无约束优化</strong>问题的首选方法，<strong>局部优化方法</strong>，混合使用牛顿法/二分法。既能保证稳定性又能快速收敛。</p><p>fmin()：SciPy.optimize 模块中求解<strong>多变量无约束优化</strong>问题的首选方法，<strong>局部优化方法</strong>，不要求目标函数是凸函数。采用下山单纯形方法。下山单纯形方法又称 Nelder-Mead 法，只使用目标函数值，不需要导数或二阶导数值,适合目标函数不可导或梯度难以计算的情况。但是，因为它不使用任何梯度评估，所以可能需要更长的时间才能找到最小值。</p><p>leatsq()：求解非线性最小二乘拟合问题，<strong>局部优化方法</strong>。</p><p>minimize()： SciPy.optimize 模块中求解多变量优化问题的通用方法，可以调用多种算法，支持约束优化和无约束优化，支持等式和不等式约束。</p><p>differential_evolution():<strong>全局优化</strong>方法，无导数优化方法，适用于目标函数不可导或导数难以计算的情况，鲁棒性强，直接支持约束优化。</p><p>basinhopping():<strong>全局优化</strong>方法，无导数优化方法，适用于目标函数不可导或导数难以计算的情况，鲁棒性强，不直接支持约束优化。</p></blockquote><h3 id="scipyoptimizeminimize"><a class="markdownIt-Anchor" href="#scipyoptimizeminimize"></a> scipy.optimize.minimize</h3><blockquote><p>求解函数为<strong>凸函数</strong>时，所求结果为<strong>最小值</strong>。求解函数为<strong>非凸函数</strong>时，只能求解<strong>局部最优解</strong>。</p><p>约束条件中默认为大于等于约束  。</p><p>在非线性优化问题通常对初始猜测值敏感，不同的初始猜测可能导致不同的结果。</p><p>初始猜测应满足所有约束条件，否则可能导致优化失败。</p><p>constraints: 一个字典（单个约束）或一个包含多个字典的列表/元组（多个约束）</p><p>Bounds:变量上下界的元组的列表/元组</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.optimize <span class="keyword">import</span> minimize</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">c1=np.array([<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">2</span>]); c2=np.array([-<span class="number">8</span>,-<span class="number">2</span>,-<span class="number">3</span>,-<span class="number">1</span>,-<span class="number">2</span>])</span><br><span class="line">A=np.array([[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">6</span>],</span><br><span class="line">            [<span class="number">2</span>,<span class="number">1</span>,<span class="number">6</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">5</span>]])</span><br><span class="line">b=np.array([<span class="number">400</span>,<span class="number">800</span>,<span class="number">200</span>,<span class="number">200</span>])</span><br><span class="line">obj=<span class="keyword">lambda</span> x: -c1@x**<span class="number">2</span>+(-c2)@x</span><br><span class="line">cons=&#123;<span class="string">&#x27;type&#x27;</span>:<span class="string">&#x27;ineq&#x27;</span>,<span class="string">&#x27;fun&#x27;</span>:<span class="keyword">lambda</span> x:b-A@x&#125;</span><br><span class="line">bd=[(<span class="number">0</span>,<span class="number">99</span>) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(A.shape[<span class="number">1</span>])]</span><br><span class="line">res=minimize(obj,np.ones(<span class="number">5</span>),constraints=cons,bounds=bd)</span><br><span class="line">res</span><br></pre></td></tr></table></figure><h3 id="scipyoptimizedifferential_evolution-basinhopping"><a class="markdownIt-Anchor" href="#scipyoptimizedifferential_evolution-basinhopping"></a> scipy.optimize.differential_evolution &amp; basinhopping</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.optimize <span class="keyword">import</span> differential_evolution, basinhopping</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fun_nonconvex</span>(<span class="params">x</span>): </span><br><span class="line">    <span class="keyword">if</span> x&lt;<span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> ( x + <span class="number">2</span> ) ** <span class="number">2</span> + <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> ( x - <span class="number">2</span> ) ** <span class="number">2</span> + <span class="number">2</span></span><br><span class="line"></span><br><span class="line">res_differential_evolution = differential_evolution(func=fun_nonconvex, bounds=[(-<span class="number">10</span>,<span class="number">10</span>)])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;differential_evolution()的结果为：\n&#x27;</span>, res_differential_evolution)</span><br><span class="line"></span><br><span class="line">res_basinhopping = basinhopping(func=fun_nonconvex, x0=<span class="number">0</span>, niter=<span class="number">1000</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;\n basinhopping()的结果为：\n&#x27;</span>, res_basinhopping)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="整数规划"><a class="markdownIt-Anchor" href="#整数规划"></a> 整数规划</h2><h3 id="线性整数规划"><a class="markdownIt-Anchor" href="#线性整数规划"></a> 线性整数规划</h3><blockquote><p>scipy库不能求解如背包问题的0-1规划问题，或整数规划问题，混合整数规划问题。Pulp库是一个用于线性规划（LP）、整数线性规划（ILP）和混合整数线性规划（MILP）问题的Python库，支持调用商业求解器处理大规模问题(留坑)。</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pulp</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 创建一个线性规划问题，最大化总回报</span></span><br><span class="line">model = pulp.LpProblem(<span class="string">&quot;Maximize_Return&quot;</span>, pulp.LpMaximize)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义决策变量</span></span><br><span class="line">x_1 = pulp.LpVariable(<span class="string">&#x27;x_1&#x27;</span>, lowBound=<span class="number">0</span>, cat=<span class="string">&#x27;Integer&#x27;</span>)   </span><br><span class="line">x_2 = pulp.LpVariable(<span class="string">&#x27;x_2&#x27;</span>, lowBound=<span class="number">0</span>, cat=<span class="string">&#x27;Integer&#x27;</span>)   </span><br><span class="line">x_3 = pulp.LpVariable(<span class="string">&#x27;x_3&#x27;</span>, lowBound=<span class="number">0</span>, cat=<span class="string">&#x27;Integer&#x27;</span>)   </span><br><span class="line">x_4 = pulp.LpVariable(<span class="string">&#x27;x_4&#x27;</span>, lowBound=<span class="number">0</span>, cat=<span class="string">&#x27;Integer&#x27;</span>)   </span><br><span class="line"> </span><br><span class="line"><span class="comment"># 建立目标函数</span></span><br><span class="line">model += <span class="number">800</span> * x_1 + <span class="number">600</span> * x_2 + <span class="number">700</span> * x_3 + <span class="number">500</span> * x_4, <span class="string">&quot;Total Return&quot;</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 建立约束条件</span></span><br><span class="line">model += <span class="number">500</span> * x_1 + <span class="number">300</span> * x_2 + <span class="number">400</span> * x_3 + <span class="number">200</span> * x_4 &lt;= <span class="number">1000</span>, <span class="string">&quot;Budget Constraint&quot;</span></span><br><span class="line">model += x_3 == <span class="number">0</span>, <span class="string">&quot;Risk Constraint for Project 3&quot;</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 解决问题</span></span><br><span class="line">model.solve()</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 查看解的状态</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Status:&quot;</span>, LpStatus[prob.status])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出每个变量的最优值</span></span><br><span class="line"><span class="keyword">for</span> variable <span class="keyword">in</span> model.variables():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;variable.name&#125;</span> = <span class="subst">&#123;variable.varValue&#125;</span>&quot;</span>)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 输出最优解的目标函数值</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;F3(x) =&quot;</span>, pulp.value(model.objective))</span><br></pre></td></tr></table></figure><h3 id="非线性整数规划"><a class="markdownIt-Anchor" href="#非线性整数规划"></a> 非线性整数规划</h3><h4 id="gekko"><a class="markdownIt-Anchor" href="#gekko"></a> Gekko</h4><blockquote><p>Gekko是一个Python库，用于解决优化问题，包括线性和非线性规划、整数规划、动态优化等。Gekko提供了易于使用的接口，并支持多种求解器，包括APOPT、BPOPT和IPOPT等。</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> gekko <span class="keyword">import</span> GEKKO</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个Gekko模型</span></span><br><span class="line">m = GEKKO(remote=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义两个整数变量x和y，并设置它们的上下界</span></span><br><span class="line">x = m.Var(integer=<span class="literal">True</span>, lb=<span class="number">0</span>, ub=<span class="number">4</span>)</span><br><span class="line">y = m.Var(integer=<span class="literal">True</span>, lb=<span class="number">0</span>, ub=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义目标函数</span></span><br><span class="line">m.Obj(x**<span class="number">2</span> + y**<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义约束条件</span></span><br><span class="line">m.Equation(x + y &lt;= <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 求解问题</span></span><br><span class="line">m.options.SOLVER = <span class="number">1</span>  <span class="comment"># 使用 APOPT 求解器（适合混合整数非线性问题）</span></span><br><span class="line">m.solve(disp=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Results&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;x: &#x27;</span> + <span class="built_in">str</span>(x.value[<span class="number">0</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;y: &#x27;</span> + <span class="built_in">str</span>(y.value[<span class="number">0</span>]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Objective: &#x27;</span> + <span class="built_in">str</span>(m.options.objfcnval))</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="cvxpy"><a class="markdownIt-Anchor" href="#cvxpy"></a> CVXPY</h4><blockquote><p>用于解决凸优化问题（整数规划、01规划和混合规划）的Python库。对于凸函数的非线性规划，cvxpy包相对scipy,pulp更专业，功能也更强大。</p><p>部分博客说不能pip install，从实践来看有误。</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cvxpy <span class="keyword">as</span> cp</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"> </span><br><span class="line">m = <span class="number">15</span></span><br><span class="line">n = <span class="number">10</span></span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">s0 = np.random.randn(m)</span><br><span class="line">lamb0 = np.maximum(-s0, <span class="number">0</span>)</span><br><span class="line">s0 = np.maximum(s0, <span class="number">0</span>)</span><br><span class="line">x0 = np.random.randn(n)</span><br><span class="line">A = np.random.randn(m, n)</span><br><span class="line">b = A @ x0 + s0</span><br><span class="line">c = -A.T @ lamb0</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Define and solve the CVXPY problem.</span></span><br><span class="line">x = cp.Variable(n)</span><br><span class="line">prob = cp.Problem(cp.Minimize(c.T@x),</span><br><span class="line">                 [A @ x &lt;= b])</span><br><span class="line">prob.solve()</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Print result.</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nThe optimal value is&quot;</span>, prob.value)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;A solution x is&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(x.value)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;A dual solution is&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(prob.constraints[<span class="number">0</span>].dual_value)</span><br></pre></td></tr></table></figure><h2 id="多目标规划"><a class="markdownIt-Anchor" href="#多目标规划"></a> 多目标规划</h2><blockquote><p>多目标规划（Multi-Objective Optimization, MOO） 是一种优化方法，旨在同时优化多个目标函数。与单目标优化不同，多目标规划通常没有唯一的最优解，而是存在一组帕累托最优解（Pareto Optimal Solutions），这些解在多个目标之间实现了最佳权衡。</p></blockquote><h3 id="线性加权和法"><a class="markdownIt-Anchor" href="#线性加权和法"></a> 线性加权和法</h3><blockquote><p>线性加权和法对每个目标函数赋予一个权重, 从而把多目标规划转化为单目标规划。</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.optimize <span class="keyword">import</span> minimize</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义目标函数（加权求和）</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">objective</span>(<span class="params">x</span>):</span><br><span class="line">    f1 = x[<span class="number">0</span>]**<span class="number">2</span> + x[<span class="number">1</span>]**<span class="number">2</span>  <span class="comment"># 目标1: 最小化 x[0]^2 + x[1]^2</span></span><br><span class="line">    f2 = (x[<span class="number">0</span>] - <span class="number">1</span>)**<span class="number">2</span> + (x[<span class="number">1</span>] - <span class="number">1</span>)**<span class="number">2</span>  <span class="comment"># 目标2: 最小化 (x[0]-1)^2 + (x[1]-1)^2</span></span><br><span class="line">    w1, w2 = <span class="number">0.5</span>, <span class="number">0.5</span>  <span class="comment"># 权重</span></span><br><span class="line">    <span class="keyword">return</span> w1 * f1 + w2 * f2  <span class="comment"># 加权求和</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义约束条件</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">constraint</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x[<span class="number">0</span>] + x[<span class="number">1</span>] - <span class="number">1</span>  <span class="comment"># 约束: x[0] + x[1] = 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始值</span></span><br><span class="line">x0 = [<span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 求解</span></span><br><span class="line">solution = minimize(objective, x0, constraints=&#123;<span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;eq&#x27;</span>, <span class="string">&#x27;fun&#x27;</span>: constraint&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;最优解:&quot;</span>, solution.x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;目标函数值:&quot;</span>, solution.fun)</span><br></pre></td></tr></table></figure><h3 id="主要目标法epsilon-约束方法"><a class="markdownIt-Anchor" href="#主要目标法epsilon-约束方法"></a> 主要目标法(<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">ϵ</span></span></span></span> -约束方法)</h3><blockquote><p>选择一个目标作为主目标，其他目标转化为约束条件</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.optimize <span class="keyword">import</span> minimize</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义主要目标函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">primary_objective</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x[<span class="number">0</span>]**<span class="number">2</span> + x[<span class="number">1</span>]**<span class="number">2</span>  <span class="comment"># 主要目标: 最小化 x[0]^2 + x[1]^2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义次要目标函数的约束条件</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">secondary_constraint</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> - ((x[<span class="number">0</span>] - <span class="number">1</span>)**<span class="number">2</span> + (x[<span class="number">1</span>] - <span class="number">1</span>)**<span class="number">2</span>)  <span class="comment"># 约束: (x[0]-1)^2 + (x[1]-1)^2 &lt;= 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始值</span></span><br><span class="line">x0 = [<span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义约束条件</span></span><br><span class="line">constraints = [&#123;<span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;ineq&#x27;</span>, <span class="string">&#x27;fun&#x27;</span>: secondary_constraint&#125;]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 求解</span></span><br><span class="line">solution = minimize(</span><br><span class="line">    primary_objective,  <span class="comment"># 主要目标函数</span></span><br><span class="line">    x0,  <span class="comment"># 初始值</span></span><br><span class="line">    constraints=constraints  <span class="comment"># 约束条件</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;最优解:&quot;</span>, solution.x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;主要目标函数值:&quot;</span>, solution.fun)</span><br></pre></td></tr></table></figure><h2 id="最大最小化规划"><a class="markdownIt-Anchor" href="#最大最小化规划"></a> 最大最小化规划</h2><blockquote><p>最大最小化规划（Maximin Optimization）是一种特殊的优化问题，其核心目标是最大化最小值。在这种规划中，我们寻找一个解决方案，使得所有可能结果中的最小值尽可能大。这种方法特别适用于那些需要确保最坏情况下的最佳性能的场景，例如资源分配、公平性问题等。</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.optimize <span class="keyword">import</span> minimize</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义目标函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">objective</span>(<span class="params">x</span>):</span><br><span class="line">    t = x[<span class="number">2</span>]  <span class="comment"># 辅助变量 t</span></span><br><span class="line">    <span class="keyword">return</span> -t  <span class="comment"># 最大化 t，等价于最小化 -t</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义约束条件</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">constraint1</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x[<span class="number">0</span>] + <span class="number">2</span> * x[<span class="number">1</span>] - x[<span class="number">2</span>]  <span class="comment"># f1(x) &gt;= t</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">constraint2</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">3</span> * x[<span class="number">0</span>] + x[<span class="number">1</span>] - x[<span class="number">2</span>]  <span class="comment"># f2(x) &gt;= t</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">constraint3</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">10</span> - (x[<span class="number">0</span>] + x[<span class="number">1</span>])  <span class="comment"># x1 + x2 &lt;= 10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始值</span></span><br><span class="line">x0 = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]  <span class="comment"># [x1, x2, t]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义约束条件</span></span><br><span class="line">constraints = [</span><br><span class="line">    &#123;<span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;ineq&#x27;</span>, <span class="string">&#x27;fun&#x27;</span>: constraint1&#125;,  <span class="comment"># f1(x) &gt;= t</span></span><br><span class="line">    &#123;<span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;ineq&#x27;</span>, <span class="string">&#x27;fun&#x27;</span>: constraint2&#125;,  <span class="comment"># f2(x) &gt;= t</span></span><br><span class="line">    &#123;<span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;ineq&#x27;</span>, <span class="string">&#x27;fun&#x27;</span>: constraint3&#125;   <span class="comment"># x1 + x2 &lt;= 10</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义变量的边界</span></span><br><span class="line">bounds = [</span><br><span class="line">    (<span class="number">0</span>, <span class="literal">None</span>),  <span class="comment"># x1 &gt;= 0</span></span><br><span class="line">    (<span class="number">0</span>, <span class="literal">None</span>),  <span class="comment"># x2 &gt;= 0</span></span><br><span class="line">    (<span class="literal">None</span>, <span class="literal">None</span>)  <span class="comment"># t 无边界</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 求解</span></span><br><span class="line">solution = minimize(</span><br><span class="line">    objective,  <span class="comment"># 目标函数</span></span><br><span class="line">    x0,  <span class="comment"># 初始值</span></span><br><span class="line">    method=<span class="string">&#x27;SLSQP&#x27;</span>,  <span class="comment"># 使用 SLSQP 算法</span></span><br><span class="line">    constraints=constraints,  <span class="comment"># 约束条件</span></span><br><span class="line">    bounds=bounds  <span class="comment"># 变量边界</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出结果</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;最优解:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x1 =&quot;</span>, solution.x[<span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x2 =&quot;</span>, solution.x[<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;t =&quot;</span>, solution.x[<span class="number">2</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;目标函数值:&quot;</span>, -solution.fun)  <span class="comment"># 最大化 t，所以取负值</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 数模 </category>
          
      </categories>
      
      
    </entry>
    
    
  
  
</search>
